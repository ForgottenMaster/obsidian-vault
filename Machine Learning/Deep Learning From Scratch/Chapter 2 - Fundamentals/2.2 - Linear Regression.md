# Introduction #

As mentioned previously, we can represent a training set as a matrix where 
each row of the matrix is a record/sample, and each column is a single feature
of the record.

Additionally we can have a weighting assigned to each feature so some features
are "worth" more than others.

However we also require a baseline value for when every numeric feature is a
0 (we would always give 0, but we may not want this).

The formula for calculating the target/output value for a given record is then
given as the following which is known as a linear regression.

$$ y_i = {\beta}_0 + {\beta}_1 \times x_1 + ... + {\beta}_n \times x_k + \epsilon $$

In this, we have a vector of weights for features and a vector of the actual values for
a record that we can add together to get the final target value for the record.

However we also include a base term (beta 0) at the beginning which is the value we use
when all features of the record are 0. This lets us shift the line of the regression to
better fit the samples.

---
# Calculating predictions without the intercept #

The first step is to think of how we can generate our "predictions" for a given
set of feature weights, and a given batch of samples. At first it helps to look at this
calculation without the additional "intercept" (the base term we add to the weighted sum).

Without the intercept, the calculation is simply a dot product, or matrix multiplication
between the vector of feature weights, and the matrix batch of samples. This will produce
a vector of predictions.

Each element of the prediction vector will be a dot product between the feature weight vector
and the corresponding row of the batch.

This is represented as:

$$ P_{batch} = X_{batch} \times W = \begin{bmatrix} X_{11} & X_{12} & X_{13} & ... & X_{1k} 
\\\\ X_{21} & X_{22} & X_{23} & ... & X_{2k} 
\\\\ X_{31} & X_{32} & X_{33} & ... & X_{3k} \end{bmatrix} \times 
\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\\\ . \\\\ . \\\\ . \\\\ W_k \end{bmatrix} 
= \begin{bmatrix} P_1 \\\\ P_2 \\\\ P_3 \end{bmatrix} $$

---
# Calculating error #

Once we have a way to calculate our predictions, we do need a way to know how
"good" those predictions are. This is just a measure of how close they are to the
target/measured numbers that we already have.

A good measure of this is the "Mean Squared Error" which is simply that we take the sum of the squared differences between the predictions and our target numbers, and divide by the number of entries to get the average difference.

A "perfect" set of weights would give us a difference of precisely 0, so the closer we get to 0 the better.

This "Mean Squared Error" function takes a vector of predictions and a vector of
targets and produces a **single number** as output and is given by the formula:

$$ MSE(P_{batch}, Y_{batch}) = MSE(\begin{bmatrix}P_1 \\\\ P_2 \\\\ P_3\end{bmatrix}, \begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3\end{bmatrix})
= \frac {(Y_1 - P_1)^2 + (Y_2 - P_2)^2 + (Y_3 - P_3)^2} 3 $$

---
# A diagram #

We can visualise the above with a little diagram. In this diagram we take the batch matrix which we call X, and the weights which we call W.

We apply the matrix multiplication in an operation we call V to get the prediction
vector P.

After we have the prediction vector P, we take in our target/result vector Y and perform the "Mean Squared Error" operation.

We represent this function with the ^ character, and the output is a single
error value known as L.

![Predictions without intercept](Calculate%20Prediction%20Without%20Intercept.png)

---
# Adding the intercept #

In order to calculate the error with the intercept (bias) included. We simply perform a scalar addition of the bias value to each element of the prediction vector just before we calculate the error.

This operation is represented with the alpha symbol in the below diagram, and the bias term represented with the character 'B' (a scalar not a vector here).

![Predictions with intercept](Calculate%20Prediction%20With%20Intercept.png)

---
# Calculating the partial derivatives for W and B #

Now that we have a way to go from the input parameters to a single value indicating our total deviation from the actual values, we need to somehow use that value to update our weights and bias terms such as to minimize L and bring it towards 0.

We can do this using everything from chapter 1 that we learned about partial derivatives.

We can figure out how much a change in each of the elements of the weighting vector, or the intercept term will cause a change in L and use that to modify those terms appropriately.

![Partial derivative calculation](Partial%20Derivatives.png)

We will work backwards when calculating the partial derivatives here (using MSE instead of ^ to indicate the mean squared error function due to LaTeX issues).

#### Partial derivative of L (with respect to P) ####

For each element in Y and P (they're the same size) then the partial derivative of the associated input element with respect to that pair is defined as the squared difference.

Summing the values and dividing by the number of elements is an operation that occurs afterwards and isn't affected by the changing of any of the input values by any amount, so the partial derivative can eliminate the sum and the division.

Since:

$$ MSE(P, Y) = (Y - P)^2 $$

We can extend this by FOIL to give us:

$$ Y_2 - (2 \times Y \times P) + P_2 $$

With this, we can see that for each element in P, if we are to increase it by
a value of 1, then it has no effect on the squared Y term. For the squared P term we can use the power rule of differentiation and see that it will become (2*P).

For the central term (-2 * Y * P), raising P by a value of 1 will raise the total by
-2 * Y.

Therefore after this, we get:

$$ \frac {\partial L} {\partial P}(P, Y) = (-2 \times Y) + (2 \times P)
= (-2 \times Y) - (-2 \times P)
= -2 \times (Y - P) $$

#### Derivative of intercept addition ####

The intercept addition operation is simply defined as

$$ \alpha = N + B $$

Then clearly increasing the value of each element of N by one unit will
increase the value of each output by 1 unit and so the derivative is just
a vector of 1's with the same shape as N.

Since it's simply an addition, then this is also the partial derivative with
respect to B. Increasing the bias term by 1 unit also increases the elements of N
by the same difference.

Therefore both partial derivatives for intercept addition are just a vector
of 1's of the same shape.

#### Derivative of matrix multiplication (with respect to W) ####
As described in chapter 1, when we have a matrix multiplication and want to
calculate the derivative with respect to one of the operands, that the result is
just the transpose of the other operand.

Therefore:

$$ \frac {\partial v} {\partial W}(X, W) = X^T$$

#### Final products ####
Now that we can calculate the partial derivatives, we can calculate the total derivative using the chain rule with respect to both W and B.

One thing to note, however is that B is a single float value, whereas the partial derivative of L with respect to P is a vector (since each element of P contributes to a change in L). We need to take into account that the bias, B is applied to each element of N to produce P. To account for the bias being applied to each element we sum to get the final derivative of L with respect to B

The total derivatives can be calculated therefore by the following formulas:

$$ \frac {\delta L} {\delta W} = {\frac {\partial N} {\partial W}} \times {{\frac {\partial P} {\partial N}} \times {\frac {\partial L} {\partial P}}} $$
$$ \frac {\delta L} {\delta B} = sum({\frac {\partial L} {\partial P}} \times {\frac {\partial P} {\partial B}}) $$---
# Using the derivatives to train #

Now that we've passed the data through the formula to get the mean squared error (L), and then done a backwards pass to get the derivatives of L with respect to W and B, we need to update W and B themselves in order to bring L towards 0.

This is a simple process and just involves us subtracting some proportion of the derivative to W and B. This proportion/multiplier is known as the **learning rate**
and makes it so we don't make too large a jump each time in the weights.

For training, we simply repeat the forward pass, backward derivative calculation, and weight updating over and over in a loop for a number of iterations known as **epochs**

---
# Training set and testing set #

There is a problem when it comes to training linear regressions and neural networks of *overfitting*. This is the problem where a set of weights have been determined that work very well in producing the target output for a given entry in the sample, but which doesn't then generalise and correctly predict for entries that haven't been seen yet.

In order to solve this issue we break up the data set into two sets.

The training set is a data set that is used to train the weights - it's the data that is passed through the formula, and for which we calculate derivatives to update the weights.

The testing set is a data set that is not used to update the weights, and is data that the network/regression hasn't had passed through it at all yet. This lets us test whether the trained network will generalise well enough to entries that it hasn't seen yet.

---
# Code Example #

The coded example of the above linear regression concepts can be found in the zip file [[Linear Regression.zip|HERE]]. As with most of the code I write nowadays, it is in Rust and will require installation of Cargo to run. The easiest way to install Rust and Cargo is through [Rustup](https://rustup.rs/)

The data set I am using for this linear regression can be found [HERE](https://www.openml.org/d/31) and is a set of features about people applying for credit, along with whether they are deemed "good" or "bad" for the purposes of credit (the target feature).

It should be fairly easy to understand and is just a codification of the above method with a few points to note:

1. Instead of training for a number of epochs, I'm training until the delta between the previous and new mean squared error values drops to within a given threshold (it's settling around the final position).
2. I am dividing the input data by the maximum value in the array to bring all values within 0 and 1 since the initial large values cause the squared error calculation to go off to infinity. Since the target values are 0 (bad) and 1 (good) we want all the values and weights to be close to this too.
3. I split the input data into 3/4 training data and 1/4 testing data. Once training has determined the final weights and intercept, I apply to the testing data to see if there's a similar error when applied to the data the system hadn't seen yet.

After running this code, I got these values (on this particular run):

```
Final error (training): 0.20486995882901385
Final error (testing): 0.20561283878025513
```

This shows that the linear regression at least seems to work with this data set as there's a very small difference in error between the training batch and the testing batch.
