After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.

The next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a *vector*.

Given the following block diagram for a chain of functions

![Vector Input Diagram](Diagram%20Of%20A%20Chain%20Of%20Functions.png)

That is, a function, f which takes two inputs both of which are vectors (denoted by the bar above the name), and a function, g which operates on the resulting value.

Furthermore, let's say that f is actually the dot product function. This function is written as follows, if we assume that X and W are both vectors of length 3:

$$ f(X, W) = X \cdot W = X_1 \times W_1 + X_2 \times W_2 + X_3 \times W_3 $$

As a reminder, the derivative of a function is the ratio of the change in the output given a change in the input, at a specific input value (point on the graph of the function). This is easy to determine when the inputs and outputs are plain values as it's just a ratio of the two deltas.

When an input is a vector though, what does a change in the input mean?. In fact, this is easily interpreted as a change in one of the *components* of the input.

As seen in the dot product formula (though the function can be any function on vectors), a vector can be decomposed into a list of individual numbers/components. Therefore we can calculate the derivative of the function with respect to an individual component by holding the others to be constant and varying the one we're interested in.

After applying this first function though, the second is operating on a single value and so the chain rule applies as before. In the case of the dot product, this gives us 6 partial derivatives: 3 with respect to the 3 components in the input vector X and 3 with respect to the components of W.

The formulas for calculating the derivatives then for the composite function will be as follows in accordance with the chain rule (where n is a valid index for the input vector):

$$ (g \circ f)\_{X\_n} = f\_{X\_n}(X, W) \times g'(f(X, W)) $$
$$ (g \circ f)\_{W\_n} = f\_{W\_n}(X, W) \times g'(f(X, W)) $$

Looking back at the formula for calculating the dot product we can see that holding all values constant except for a single component we are interested in causes a change in the output proportional to the matching component of the other vector. For example if we let $X_1$ increase by 1 while holding all other components constant, we can see that the only part of the formula containing $X_1$ is

$$ X_1 \times W_1 $$

Increasing $X_1$ by 1 then increases the output only by $W_1$

When we calculate the derivative for the other components then, we find out that elementwise, the derivatives of the dot product with respect to X can be represented by:

$$ \begin{bmatrix} W_1 & W_2 & W_3 \end{bmatrix} $$

and by the same token, the component wise derivatives of the dot product with respect to W is:

$$ \begin{bmatrix} X_1 & X_2 & X_3 \end{bmatrix} $$

When we write the dot product as row and column vectors in order to have the correct shapes for performing the dot product:

$$ \begin{bmatrix} X_1 & X_2 & X_3 \end{bmatrix} \cdot \begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \end{bmatrix} $$

We can see that the component wise derivative with respect to X will be $\begin{bmatrix} W_1 & W_2 & W_3 \end{bmatrix}$, that is $W^T$ and the component wise derivative with respect to W will be $\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \end{bmatrix}$, that is $X^T$

---
# Conclusion #

In conclusion, calculating the derivative of a function with vector inputs is just calculating the derivative of the function with respect to each component in the vector. Additionally, and most importantly for deep learning, the derivative of the dot product of a row vector X with a column vector W turns out to be $W^T$ and $X^T$ respectively.
