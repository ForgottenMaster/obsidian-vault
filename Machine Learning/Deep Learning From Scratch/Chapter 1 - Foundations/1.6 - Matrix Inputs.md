The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.

In the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.

---
# Matrix Multiplication Refresher #

The first step is to recall how we can multiply two matrices together. In deep learning typically this will be performed with a matrix of samples, and a matrix of weights to be applied. The result being a weighted sum of the samples with weights.

In order for the product of two matrices to be defined, the number of columns in the first matrix must equal the number of rows in the second. The resulting matrix will be of the outer dimensions.

That is, given a matrix X, of dimensions **m x n** and a matrix W, of dimensions **n x p** then the multiplication is defined, and the resulting matrix is of dimensions **m x p**.

In the resulting matrix, an element at row i and column j is the result of the dot product of the ith row of the first matrix, and the jth row of the second matrix.

As a concrete example, suppose we have

$$ X = \begin{bmatrix} X_{11} & X_{12} & X_{13} \\\\ X_{21} & X_{22} & X_{23} \\\\ X_{31} & X_{32} & X_{33} \end{bmatrix} $$
$$ W = \begin{bmatrix} W_{11} & W_{12} \\\\ W_{21} & W_{22} \\\\ W_{31} & W_{32} \end{bmatrix} $$

Then the resulting matrix can be represented as

$$ X \cdot W = \begin{bmatrix} XW_{11} & XW_{12} \\\\ XW_{21} & XW_{22} \\\\ XW_{31} & XW_{32} \end{bmatrix} $$

Where $XW_{ij}$ is the dot product of the ith row of X and the jth row of W

---
# Derivative Of Matrices #

In order for us to be able to calculate the rate of change in the output of a function with respect to the elements of one of the input matrices we will need to have the output be either:

1) A matrix of the same size as an input matrix - derivative can be calculated elementwise in that case between input and output matrices
2) A scalar value - derivative can be calculated elementwise for each element in the input matrix to determine how this affects the single output

For deep learning, we want to be able to first multiply a matrix of samples (X) by a matrix of weights (W) in order to get the weighted sum of the records in a resulting matrix, then we would like to pass the resulting matrix to a function that produces a single value that could be used to activate a neuron for example.

Therefore what we have is a function F which can be defined as 

$$ F = F(Y) $$

That is it takes some matrix Y and performs an operation on it (in our case likely summing the elements). The matrix Y will be produced by multiplying the two input matrices, that is:

$$ Y = X \cdot W $$

In order to calculate the total derivative with respect to the input matrices, we can apply the chain rule. The first step of which is easy, that is to take Y and calculate how a change in each element would affect the resulting single value. The derivative of F therefore is a matrix representing these partial derivatives. We will call this G, that is $G = F'(Y)$

The derivative of Y depends on two variables X and W which are multiplied together. The formula for the total derivative therefore is

$$ dY = dX \cdot W + X \cdot dW $$

That is, changes in the elements of X would end up being multiplied by W, and also changes in the elements of W would end up being multiplied by X.

Then, the derivative of the total function can be given (using the chain rule) as:

$$ dF = G:dY $$

And expanding out dY with the above formula gives

$$ dF = G:dX \cdot W + G:X \cdot dW $$

For notation, X:Y means the elementwise multiplication of matrices X and Y, that must be the same dimensions (also known as frobenius inner product).

---
# Isolating dX and dY #

In the above formula, we have G being multiplied elementwise by a matrix that is the result of matrix multiplication between dX and W or X and dW.

What we want to do is to isolate dX and dY so that they are multiplied elementwise with the other matrix. How can we get W and X onto the other side?

Well,

$$ G : (dX \cdot W) = (G \cdot W^T) : dX $$

And

$$ G : (X \cdot dW) = (X^T \cdot G) : dW $$

That is, the total derivative can be calculated as

$$ dF = (G \cdot W^T) : dX + (X^T \cdot G) : dW $$

### Partial Derivatives ###

Now from the above formula, we can easily calculate the partial derivatives of F by holding either X or W as constant, meaning dX or dW respectively is 0.

Doing this will eliminate the term from the formula leaving only the other one. This shows us how the partial derivatives are

$$ \frac{\partial F}{\partial X} = G \cdot W^T $$
$$ \frac{\partial F}{\partial W} = X^T \cdot W $$
---
# Additional Notes #

The book at this point actually didn't explain how we get to using the transpose matrices for derivatives, instead the book says in a handwavy fashion that "the way the mathematics works out".

I found the above explanation at [This Page](https://math.stackexchange.com/questions/2755654/partial-derivative-of-matrix-product-in-neural-network) and above tried to break it down as best I could.

I will likely need to revisit matrix multiplication in the future but for now understand it enough to move on to learning more about the structure of a neural network and applying these rules in practice.
