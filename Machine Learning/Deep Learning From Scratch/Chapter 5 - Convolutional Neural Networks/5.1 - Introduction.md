With the standard feed-forward architecture we used in the chapters until this point, we are able to input a collection of observations, with each observation having a number of features. In the case of the MNIST recognition network, these features were the greyscale pixel values.

However, such a "flat" model has no knowledge or concept of "closeness" of features to one another. This means that the learned features in a hidden layer, derived as some linear combination of the input pixel data could draw from pixels that are nowhere near each other in the input.

In a convolutional neural network, we can provide a way to learn based on data that is spatially close to each other (such as a 3x3 patch of pixels in a 2D image).

---
# Architecture #

The architecture we will be going for at a high level can be represented with the image below:

![Architecture](Architecture.png)

The basic idea is that we will take small patches of a 2 dimensional area in the original data, and the learned features will be some linear combination of those.

We therefore need a way to transform these 2-D patches into values that can then be linearly combined with the regular dense layers.

---
# Convolution Operation #

The core of a CNN is this operation that allows us to extract features from an image by applying some weight matrix as a sliding window over the original image to produce a slightly different image.

As an example, if we have a 5x5 input image defined as:

$$ I = \begin{bmatrix} 
i_{11} & i_{12} & i_{13} & i_{14} & i_{15} \\\\
i_{21} & i_{22} & i_{23} & i_{24} & i_{25} \\\\
i_{31} & i_{32} & i_{33} & i_{34} & i_{35} \\\\
i_{41} & i_{42} & i_{43} & i_{44} & i_{45} \\\\
i_{51} & i_{52} & i_{53} & i_{54} & i_{55}
\end{bmatrix} $$

And we want to extract features of each 3x3 patch in the image, we can envisage this as sliding a smaller matrix of weights across the image pixels and generating a new pixel value in an output image that is a result of the weights being applied to the surrounding pixels in the input image.

Given a 3x3 matrix of weights:

$$ W = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\\\
w_{21} & w_{22} & w_{23} \\\\
w_{31} & w_{32} & w_{33}
\end{bmatrix} $$

Then take for example the pixel in I that is centered over (3,3). We would multiply each pixel in the patch by the associated weight in the weight matrix and sum the result as in:

$$ O_{33} = w_{11} \times i_{22} + w_{12} \times i_{23} + w_{13} \times i_{24} + w_{21} \times i_{32} + w_{22} \times i_{33} + w_{23} \times i_{34} + w_{31} \times i_{42} + w_{32} \times i_{43} + w_{33} \times i_{44} $$

This feature can then be passed through activation functions or through subsequent layers in the neural network.

---
# Multichannel Convolution Operation #

The above describes a single application of some set of weights to an input image to produce an output image (AKA a feature map), under a single convolution operation.

However in a CNN, we will apply multiple convolution operations with different sets of weights.

The below image shows a representation of how an input image is transformed to an output image/feature map, but in a multichannel convolution operation this will be repeated a number of times - one for each set of weights:

![Convolution Operation](Convolution.png)

In terms of CNNs, the number of output images we're generating is called the number of channels. Each of the sets of weights is known as a **convolutional filter**