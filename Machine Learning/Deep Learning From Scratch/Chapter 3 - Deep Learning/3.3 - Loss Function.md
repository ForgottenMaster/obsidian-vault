# Overview #

Once we've passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of **predictions** as mentioned previously (1 for each observation in the data). We already have a vector of **targets** which are the actual measured answers. We know that we will need to calculated derivatives on the backpropagation pass, but how do we know what to use as an initial value?

This is where the "loss function" (or error function) comes in, which will take the vector of predictions, and a vector of targets, and will produce a single number which identifies just how **far away** our predictions are from the target values.

This loss function will be implemented in a similar fashion to how we have done Operations and Layers so far, in that it will provide the framework and public API to call for use in the forward and backward passes, as well as asserting the shapes of arrays along the way.

It will delegate the actual calculations to another vtable like trait object though for customisation.

---
# LossImpl #

First of all we define a trait for us to use to wrap a trait object. This allows the customisation point for choosing exactly what kind of calculation we would like. This trait is parameterised over the type T (type of the Array elements in use) as the others have been, but otherwise looks fairly uneventful:

```rust
pub trait LossImpl<T> {
    ...
}
```

This trait will define two functions. The first is a function that will calculate a single value (of type T of course) when given the Array of predictions and targets. The signature looks as expected from such a function:

```rust
fn calculate_output(&self, prediction: &Array<T, Ix2>, target: &Array<T, Ix2>) -> T;
```

That function will be used on the forward pass in order to calculate the single value, total "error". The other function we will need is a way of calculating what the gradients should be of the predictions (since these are what are varying and calculated) to kick off the backpropagation step. This function also takes the predictions and targets arrays, except that it will produce an Array of gradients, of the same shape as predictions:

```rust
fn calculate_input_gradient(
    &self,
    prediction: &Array<T, Ix2>,
    target: &Array<T, Ix2>,
) -> Array<T, Ix2>;
```

---
# MeanSquaredError #

This is an implementation of a loss function, and is one of the most commonly used ones, the average squared error. This is popular also because it's easy to calculate the gradient for as we'll see soon. Like the customisation points for Operation and Layer, this type doesn't need to store any state, so is defined as a simple unit struct:

```rust
pub struct MeanSquaredError;
```

Next we will need to implement the LossImpl trait for it so we can use it as a trait object. We need to place some generic bounds on this though for T, so I'll show the impl block first, and then explain why we need the bounds:

```rust
impl<T: LinalgScalar + From<u32> + ScalarOperand> LossImpl<T> for MeanSquaredError {
}
```

1. **LinalgScalar** - This trait is required for the same reason as we needed it in our Operation implementations. It allows for us to use arithmetic operations on Array's of this type, which will be required to calculate the mean squared error.
2. **From\<u32>** - We need to be able to convert from numbers into our type T at a couple of points. One is when calculating the gradient, we need to be able to multiply the error by **2**. However most types implement multiplication with both operands as the same type, therefore we need to be able to convert the integer 2 into an instance of type T. Additionally, we want to divide by the number of entries, which again should be an instance of type T. The downside here is that array.nrows() will give a usize, but we use From\<u32> instead because floating point types implement it not conversion from usize. The program will crash if there are more than u32 observations in the data.
3. **ScalarOperand** - This is required because we multiply an array by a scalar in calculating the input gradient.

With that out of the way, we can implement the two functions!

First up is the calculate_output function. In the case of the mean squared error, this is just finding the difference between prediction and target arrays, then squaring that difference (we can use the elementwise multiplication operator * on the error array). Then we take the squared error Array and find its sum. Finally we divide by the number of rows. The code is fine to follow so I'll just post it here:

```rust
fn calculate_output(&self, prediction: &Array<T, Ix2>, target: &Array<T, Ix2>) -> T {
    let error = prediction - target;
    let squared_error = &error * &error;
    let squared_error_sum = squared_error.sum();
    squared_error_sum / (prediction.nrows() as u32).into()
}
```

As mentioned above, in the last line you can see that in order to divide by the number of rows (count), we need to turn that number into an instance of type T. We can do this because we put the trait bound From\<u32> which allows us to use the "into" function on a u32 to get an instance of T to divide by.

The second function is also kind of simple. The formula for calculating the gradient is **2 * (prediction - target) / count**. There's another instance of converting the count into an instance of type T, however because we're also multiplying by 2, we need to get an instance of type T that represents the number "2":

```rust
fn calculate_input_gradient(
    &self,
    prediction: &Array<T, Ix2>,
    target: &Array<T, Ix2>,
) -> Array<T, Ix2> {
    let two_of: T = 2.into();
    let error = prediction - target;
    let count: T = (prediction.nrows() as u32).into();
    let average_error = error / count;
    average_error * two_of
}
```

---
# Loss #

For the actual Loss type, we would like to allow customization via specifying a LossImpl trait object as a "vtable" as previously described. Additionally, on the forward pass we will need to cache the predictions and targets in order to calculate the gradient with the last used pair on the backward pass. As with Operation and Layer, these are initially None and then on the first forward pass will be set to Some. Therefore they are of type Option. As our LossImpl trait object is a reference that can be on the stack, we will require a lifetime annotation ('a in the code). The full definition then looks as follows:

```rust
pub struct Loss<'a, T> {
    implementation: &'a dyn LossImpl<T>,
    prediction: Option<Array<T, Ix2>>,
    target: Option<Array<T, Ix2>>,
}
```

The first function we will need is a way to construct a new instance. We will do this with the idiomatic "new" function. The only thing we are required to provide when constructing one is the LossImpl trait object as the other two fields will be initialized to None like so:

```rust
pub fn new(implementation: &'a dyn LossImpl<T>) -> Self {
    Self {
        implementation,
        prediction: None,
        target: None,
    }
}
```

The second function we need is a function to call on the forward pass to calculate the total loss value. For this we will pass it the predictions and targets arrays, which are required to be the same size. We delegate the actual output calculation to the LossImpl provided. We will also stash the provided predictions and targets for use in the backward pass. The full function is fairly simple so I'll list it here:

```rust
pub fn forward(&mut self, prediction: Array<T, Ix2>, target: Array<T, Ix2>) -> T {
    assert_eq!(prediction.raw_dim(), target.raw_dim());
    let loss_value = self.implementation.calculate_output(&prediction, &target);
    self.prediction = Some(prediction);
    self.target = Some(target);
    loss_value
}
```

The final method we need is to perform the backwards pass. This is also simple and just passes the previously stashed predictions and targets from the forward pass into the input gradient calculation function of LossImpl. We can use the standard "as_ref().unwrap()" pattern on Option\<T> to get a &T from it. After we get the input gradient back from the implementation, we need to check that the shape matches that of the predictions since we'll be starting the backpropagation with it. The function implementation is then as follows:

```rust
pub fn backward(&mut self) -> Array<T, Ix2> {
    let input_gradient = self.implementation.calculate_input_gradient(
        self.prediction.as_ref().unwrap(),
        self.target.as_ref().unwrap(),
    );
    assert_eq!(
        self.prediction.as_ref().unwrap().raw_dim(),
        input_gradient.raw_dim()
    );
    input_gradient
}
```