# Overview #

The lowest level of abstraction as mentioned in the previous post is the [[3.1 - Operations|Operation]]. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.

During that chapter, we could see how multiple of these operations are able to be chained together, though we don't want to do this by hand. Instead, we would like an abstraction that has the same interface as the operations (a forward and backward method) but which internally handles routing the data through the operations that are part of the layer.

We define a layer as a series of linear operations, followed by a final (optional) non-linear operation known as the "activation function".

In the context of the simple neural network implemented in the last chapter, we had 5 operations that were:

1. A **WeightMultiply** operation which takes the initial input and selection of weights and multiplies them together, ending with a matrix that represents the weighted sum for each of the "learned features" that we wanted in this "hidden layer".
2. A **BiasAdd** operation which added in the bias term to the weighted multiplication.
3. A **Sigmoid** function that took the results and "squashed" them to a non-linear range between 0 and 1.
4. A **WeightMultiply** operation which took the inputs from the learned features, and multiplied them with a second set of weights.
5. A **BiasAdd** operation which added in the bias term to the calculated weighted sum (which was a single value at this point).

We can see that there are 2 layers here:

1. Operations 1 to 3 comprise the first layer which takes the initial input, operates on it, and produces values for a set of learned features (or as we'll use here, "neurons")
2. Operations 4 and 5 will comprise the final layer which takes the learned hidden features to the final output

A diagram below shows this grouping of operations (with the loss function tacked onto the end):

![Layers Diagram](Neural%20Network%20Layers%20Diagram.png)

---
# Neurons #

Rather than looking at the individual operations within the layers, we instead look at neural networks at the abstraction of these layers. A layer is, as mentioned previously a series of non-linear operations followed by a linear one, but another important facet of a layer is the number of *neurons* it contains.

In the brain, neurons will take electrical impulses from other connected neurons and will *fire* if that energy exceeds some certain threshold (which then causes other neurons to possibly fire, etc.). In the case of artificial neural networks, this activation is modeled by transformation of the input to an output through a non-linear function (the Sigmoid function for example), which we also can call the *activation function* for a layer. The values coming out of this are the *activations* for that layer.

The number of neurons in an artificial neural network will correspond to the number of features in the input. Therefore in the simple neural network seen in the last chapter, and whose operations are briefed above we can see that:

1. The number of neurons in the **input** layer is X where X is the number of features in the data set.
2. The number of neurons in the **hidden** layer is N where N is some number that we define. The output from this layer will be a matrix with the number of rows as in the input, and the number of columns equal to the neuron count for that layer.
3. The number of neurons from the **output** layer is 1. This is because we're collapsing the values down to a single prediction for each observation (to allow us to use the loss function to detect error).

Deep learning is simply using neural networks with **more than one** hidden layer.

A more useful diagram below doesn't show the operations inside the layers, but abstracts the layers only to show their inputs and outputs:

![Layers Abstraction](Neural%20Network%20Layers%20Abstraction.png)

---
# A small detour #

Before delving into the implementation for the Layer abstraction, we first address a need for a function that will be required to be added to the "Operation" type that we implemented in the previous section.

For ParameterOperation instances we stored the parameter being used, along with the last computed gradient for the parameter in the backward pass. One thing that we will need and will lift to the Layer abstraction is the ability to access temporarily the parameter and parameter gradients in use.

In order to achieve this, we will add the following function to the Operation type:

```rust
pub fn parameter_and_gradient(&mut self) -> Option<(&mut Array<T, Ix2>, &Array<T, Ix2>)> {
    ...
}
```

This function will return the parameter as a mutable reference, since we will eventually need to be able to update the parameter, given the gradient. However the gradient doesn't need to be mutable so we return an immutable reference.

One thing to note here is that not all operations have a secondary parameter (those that are BasicOperations) - therefore we return an Option to allow for this possibility.

The implementation of this function then look as follows:

```rust
match &mut self.data {
    OperationData::Parameter {
        parameter,
        parameter_gradient,
        ..
    } => Some((parameter, parameter_gradient.as_ref().unwrap())),
    _ => None,
}
```

That is, we must match on the OperationData enum value we have. In the case of it being a parameter operation, we can get the references we want, and return these in a "Some" variant. If not, then we return None as there's no parameters to get.

---
# LayerSetup #

Just as we have the BasicOperation and ParameterOperation traits to allow for customization of the calculations for specific operations, we would also like to define a static structure for a Layer while allowing construction of that layer (determining which operations, etc.) to be customized.

This is the purpose of the LayerSetup trait. We will shortly see that the structure of Layer is static while delegating the actual setup of the layer to a trait object implementing this trait.

The trait header looks as follows:

```rust
pub trait LayerSetup<'a, T> {
    ...
}
```

This again requires the same generic parameters that are required by Operation. That is:

1. **'a** - This is the lifetime of the borrow for the BasicOperation or ParameterOperation vtables in use
2. **T** - This is the type of the elements inside the ndarray::Array

Looking at the trait itself, it only requires implementors to provide the following function:

```rust
fn layer_setup(
    &self,
    num_neurons: usize,
    input: &Array<T, Ix2>,
    seed: &Option<u64>,
) -> Vec<Operation<'a, T>>;
```

This is a function that, given the number of neurons/features that the layer should have, along with the input (used to determine the number of features in the input) can return the list of Operations required in that layer.

The additional parameter is an optional seed used for random number generation. If this is None then it's expected that the numbers should be truly randomly generated, if it's Some then the seed should be used to construct the PRNG stream.

---
# DenseLayerSetup #

Previously we saw that the layers (the hidden and output layers) in our neural network fully connects the neurons of the previous layers and consists of a sequence of **WeightMultiply -> BiasAdd -> ActivationFunction** where ActivationFunction is some non-linear function. For the hidden layer we used Sigmoid and for the output layer we used a Linear activation function which is just a passthrough function.

However the basic concept of fully connecting the neurons of the previous layer, and using a weighted sum and bias addition is consistent between layers. The activation function needs to be tweakable.

The terminology for these fully connected layers is a *dense* layer (due to it being densely connected).

This, therefore will be known as the DenseLayerSetup.

As mentioned above, we will need to modify the activation function in use, so we will keep this as a field. As before, we will need to make it generic over the lifetime, 'a, and the type T. We know that the activation function should be one that doesn't take an additional parameter (if this proves an incorrect assumption we can change later), so we can take just the BasicOperation trait object rather than a wrapped Operation.

The structure definition therefore is as follows:

```rust
pub struct DenseLayerSetup<'a, T>(&'a dyn BasicOperation<T>);
```

Next, we want a simple constructor function for this. We don't want to make the fields public since they could be changed after construction, so instead we keep the field private and provide a constructor function for external use.

```rust
impl<'a, T> DenseLayerSetup<'a, T> {
    pub fn new(activation: &'a dyn BasicOperation<T>) -> Self {
        Self(activation)
    }
}
```

Finally, we have the actual trait implementation. First of all, we have the header for the impl block:

```rust
impl<'a, T: LinalgScalar> LayerSetup<'a, T> for DenseLayerSetup<'a, T>
where
    Standard: Distribution<T>,
{
    ...
}
```

A couple of things to note here:

1. We require a trait bound of LinalgScalar on the generic parameter, T. This is because the layer needs to use the WeightMultiply and BiasAdd operations that both put a restriction of LinalgScalar on their generic parameter (due to requiring the arithmetic operations, etc.)
2. We are using random generation for the initial parameters on the weight and bias operations. In order to use this random number generation, we require that our type T is such that the structure "Standard" implements Distribution\<T>.

Next is the actual function implementation. I won't repeat the function signature here as it was specified in the LayerSetup section.

The first step in the implementation is to get the random number generator instance. This is going to depend on if there's a random seed specified or not. If there is, we will use it to construct the generator. If not, we get a truly random generator. This code looks as follows:

```rust
let mut r = match seed {
    Some(seed) => StdRng::seed_from_u64(*seed),
    None => StdRng::from_rng(thread_rng()).unwrap(),
};
```

Next, we need to generate the random weights and bias values to use as parameters to the appropriate operations. The steps for this are as follows:

1. We figure out what dimensions we require for the weights matrix. The number of rows in the weights matrix will match the number of columns/features in the input data - this allows us to perform the matrix multiplication. The number of columns in the weights matrix will then be the number of output neurons we require in the layer.
2. Calculate the number of elements in the matrix which we use for creating the buffer in the next step. This is just the number of rows multiplied by number of columns.
3. Create a vector with a number of elements calculated in step #2. Each element is a randomly generated value using the generator we made previously. In order to make these, we can use the **repeat_with** function from std::iter which takes a function and calls it repeatedly. We provide a function that just produces random values. Then we use the **take** adapter to take the appropriate number of elements, and finally collect them into a vector.
4. Finally, we use the **Array::from_shape_vec** constructor to turn the Vec into an Array.

The code for the above looks as follows:

```rust
let weight_dim = (input.ncols(), num_neurons);
let weight_count = weight_dim.0 * weight_dim.1;
let weights = repeat_with(|| r.gen::<T>()).take(weight_count).collect();
let weights = Array::from_shape_vec(weight_dim, weights).unwrap();
```

Creating the bias matrix is the same, except the dimensions are different. The bias matrix always consists of a single row, and the number of columns is the number of neurons we have. This looks as follows:

```rust
let bias_dim = (1, num_neurons);
let bias_count = bias_dim.0 * bias_dim.1;
let bias = repeat_with(|| r.gen::<T>()).take(bias_count).collect();
let bias = Array::from_shape_vec(bias_dim, bias).unwrap();
```

Finally we have the parameters we need to create the layer. We make sure to include the activation function that we stored as a field as the last operation in the sequence:

```rust
vec![
    Operation::new_parameter(&WeightMultiply, weights),
    Operation::new_parameter(&BiasAdd, bias),
    Operation::new_basic(self.0),
]
```

---
# Layer #

Finally we can implement the abstraction for a Layer. As with Operation, I'll split this up into sub-headers to break it up a little.

#### Definition ####

The structure definition looks as follows:

```rust
pub struct Layer<'a, T> {
    num_neurons: usize,
    operations: Option<Vec<Operation<'a, T>>>,
    setup: &'a dyn LayerSetup<'a, T>,
    output: Option<Array<T, Ix2>>,
    seed: Option<u64>,
}
```

As always, we need to take the type of elements in the arrays (T) and lifetime of the OperationData vtables ('a).Following is a description of each parameter within the definition however:

1. **num_neurons** - This is the number of columns/features in the output of the layer. It's used to generate weights and bias of a correct size to produce the desired dimension
2. **operations** - This is an owned vector of the Operations in the layer. It's an Option here because we can default to None until the first forward pass is performed. We could use an empty Vector but that would not be as explicit. This way if someone tries to use the list of operations without doing a forward pass first then they'll see it's invalid by the program panicking
3. **setup** - A reference to the LayerSetup to use to produce the list of Operations on the first forward pass
4. **output** - Store the calculated output from the forward pass which can be used then on a backward pass
5. **seed** - The optional random seed to use for generating weights/bias terms in the layer

#### Factory Functions ####

Secondly we have **2** factory functions to create a Layer. One of which takes the random seed to use, and the other which doesn't (so the RNG will be different every time). The basic version will take the number of neurons we want in the layer, along with a required LayerSetup implementation.

Everything else is set to None by default:

```rust
pub fn new(num_neurons: usize, setup: &'a dyn LayerSetup<'a, T>) -> Self {
    Self {
        num_neurons,
        operations: None,
        setup,
        output: None,
        seed: None,
    }
}
```

The fixed seed version is more or less the same, except it sets the seed to the provided value:

```rust
pub fn new_with_seed(num_neurons: usize, setup: &'a dyn LayerSetup<'a, T>, seed: u64) -> Self {
    Self {
        num_neurons,
        operations: None,
        setup,
        output: None,
        seed: Some(seed),
    }
}
```

#### Forward Pass ####

Both the forward and backward functions are inside an impl block that looks as follows:

```rust
impl<'a, T: Clone> Layer<'a, T> {
    ...
}
```

That is, we place the bound of Clone on the type T. The reason for this is simply that the forward method takes "input" by reference, however we will need it by value for folding as we'll see shortly, so we need to clone it.

The signature of the forward method itself is identical to the forward method on Operation. This is not a coincidence as Layer is wrapping multiple Operations while keeping the API the same. This is the *Composite* design pattern where one larger thing has the same API as the smaller unit, but which encapsulates multiple of those smaller units.

```rust
pub fn forward(&mut self, input: &Array<T, Ix2>) -> Array<T, Ix2> {
    ...
}
```

The first step on the forward pass is to make sure that we generate (using the LayerSetup) the list of Operations on the first forward pass. We can actually do this nicely in Rust by using a method on Option\<T> called **get_or_insert_with**. This function takes a callback, and calls it if the Option is currently None. The return value from the function is then set as the value of the Option for further runs. Finally a reference to the got or inserted entry is returned.

```rust
let operations = self
    .operations
    .get_or_insert_with(|| self.setup.layer_setup(self.num_neurons, input, &self.seed));
```

In order to perform the forward pass, we basically just want to start off with the input we were provided, and then pass it through the chain of operations. We can easily do this with the **fold** method on iterators. This method takes an initial state (starts as the provided input), and a function that is called on every item in the iterator that can update and return new state. Once we've gone through all elements, the final value is returned, which we'll stash in self.output:

```rust
self.output = Some(
    operations
        .into_iter()
        .fold((*input).clone(), |state, operation| {
            operation.forward(&state)
        }),
);
```

Finally we can return a clone of the output (we can't return the output itself as we need a copy for the backward pass):

```rust
(self.output.as_ref().unwrap()).clone()
```

#### Backward Pass ####

For the backward pass, just like with Operation, we will first need to validate the shape of the provided output gradient is correct before passing it **backwards** through the operations list.

The signature also is very familiar as it mimics the same for Operation:

```rust
pub fn backward(&mut self, output_gradient: &Array<T, Ix2>) -> Array<T, Ix2> {
    ...
}
```

We firstly verify that the shape of the provided output gradient matches that of the output of the layer, as calculated from the forward pass:

```rust
assert_eq!(
    self.output.as_ref().unwrap().raw_dim(),
    output_gradient.raw_dim()
);
```

Passing the output gradient through the series of Operations to get an input gradient to return is the same as the forward pass almost. We do need to remember to run *backwards* through the vector of operations. We can get a reversed iterator with the **rev()** method:

```rust
self.operations
    .as_mut()
    .unwrap()
    .into_iter()
    .rev()
    .fold((*output_gradient).clone(), |state, operation| {
        operation.backward(&state)
    })
```

#### Parameters And Gradients ####

Finally we need a way to process all the parameters and gradients from ParameterOperations on the layer. This is why we added the parameter_and_gradient method to Operation, so that we can access those references from the layer.

This function is a bit tricky to write however, because the reference to the parameter is mutable.

Rust doesn't allow multiple mutable references to the same item (or even a mutable and immutable reference active at the same time) and so we can't just return references as returned from parameter_and_gradient. This is because those are tied to the lifetime of the Operation itself, rather than the lifetime of the iterator.

To convince Rust that the returned references have the same lifetime as the iterator, we can specify explicitly that the references have the same lifetime as the **&mut self** parameter to the method. The iterator owns the &mut self borrow until it goes out of scope, so we can tie the lifetime of the references returned as items to the lifetime as the borrow of Self fairly easy:

```rust
pub fn parameters_and_gradients<'b>(&'b mut self) -> impl Iterator<Item = (&'b mut Array<T, Ix2>, &'b Array<T, Ix2>)> {
    ...
}
```

Here we are introducing the lifetime parameter 'b, and saying that this borrow **&mut self** lasts for that length of time. Then we're saying that the items in the returned iterator also live for this time too.

The next step is that we require a little unsafe....

The problem we have now is that the vector of Operation instances we have is of the following type:

**Vec<Operation<'a, T>>**

Now 'a is the lifetime of the Operation itself (or more specifically the lifetime of the & dyn BasicOperation/& dyn ParameterOperation references inside). So if we were to iterate over that as it is now, then it would produce references with lifetime 'a.

This means the produced references from the iterator would live for 'a, but the iterator itself only for 'b. Thus, there's a conflict and the compiler will complain.

In order to solve this, we can use (unsafely) std::mem::transmute to *very carefully* change Operation<'a, T> into Operation<'b, T>.

Now this is perfectly safe in fact because we know that the references *actually* live for 'a, which is guaranteed to live at least as long as 'b. Therefore there's no harm in shortening the lifetime to 'b on those Operation instances.

The code for that is as follows (note that I'm coercing the Vec into a slice and transmuting that, but either works):

```rust
let operations = unsafe {
    transmute::<&'b mut [Operation<'a, T>], &'b mut [Operation<'b, T>]>(
        self.operations.as_mut().unwrap(),
    )
};
```

Once we've done this transmutation to shorten the invariant lifetime on Operation, the rest of the function is simple enough and is just a case of doing a **filter_map** and calling parameter_and_gradient on each operation.

parameter_and_gradient returns None if it's a BasicOperation, or Some((param, grad)) if it's a ParameterOperation so we can return this from filter_map directly.

filter_map will only pass forward any Some values and will unwrap them as it runs through them:

```rust
operations
    .into_iter()
    .filter_map(|op| op.parameter_and_gradient())
```