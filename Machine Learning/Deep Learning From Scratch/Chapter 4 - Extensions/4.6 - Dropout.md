Out of all the modifications for neural networks we've covered in order to try to get the network to train the problem "better", we haven't actually tried what people would think is the most obvious - adding more neurons to the network.

---
# Introduction #

Adding more neurons to the network can take one of two forms:

1. Adding more neurons to a hidden layer in the network (expanding wide)
2. Adding more hidden layers to the network (expanding long)

More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some "learned feature" that the network deems important.

With a linear regression, we only have a single neuron which means only a single parameter we can tweak in order to change the prediction. It's clear that there are some numbers in a linear regression that just can't possibly be made because we only have a single multiplier and bias term (y = mx + c).

So if fewer data points means that the network is less accurate then surely adding more neurons to the network means it will be *more accurate*, right?

Well....yes, and no. It's true that more neurons means we can model more complex graphs, however it's also true that the network will **overfit** to the training data, and not perform very well at all when used on testing data, or "in the wild".

Dropout is a way to let us add additional capacity (extra neurons) to the network, while also making it less likely that the network overfits the training data.

---
# What is dropout? #

How do we avoid the network overfitting the training data?. We simply need to randomly set neurons in the network back to **0**, or remove their contribution from the predictions in each epoch.

The intuition here is that it prevents a neuron's weight from "locking on" to a particular value and keeping it for the entirety of training. When we set a neuron's weight back to 0 then it must recalculate it from other neurons in the network in the optimisation step.

This continuous set back of random neurons, and their subsequent recalculation from other neurons means that the network ends up not overfitting (or at least less likely). After all, how can you overfit the data if you keep changing?

One thing to remember though, is when we're making predictions for the purposes of testing, or when being used in the wild to predict using a trained network - we do **not** want to use dropout, since that changes the state of the network that we've worked so hard to train!

However, during training, dropout ends up reducing the total **magnitude** of the values that are output from the layer by an average of (1-p) where p is the probability of dropping the neuron.

This makes sense, because when we dropout a neuron and set it to 0, the entire contribution to the weighted sum from that neuron is dropped to 0. The total weighted sum from the layer which was calculated from M neurons is now only calculated from **$M \times (1 - p)$** neurons. When averaging this, it means that if the chance of dropout is 30% for example, it means that 30% of the overall sum passing forward is reduced by 30%.

Therefore, when we're making predictions, we want to not drop out the neuron, but we **do** want to simulate this reduction in magnitude so as to keep the values going forward at the expected range to not give false answers. We'll do that by allowing the caller to specify they're using **inference** mode, and we'll just simulate the dropout by multiplying the whole output by the keep probability.

---
# Implementing Dropout #

In order to implement dropout, we'll create a BasicOperation that is tacked onto the end of the operation sequence of the layer when we create the layer.

However there is a minor problem, as we'll soon see that will require a little refactor to our code in order to allow operations to retain state.

Up until now, our operations were stateless and have just been used as a vtable, however in order for us to apply the **same** dropout mask as calculated in the forward pass (remember this is generated randomly each forward pass), we'll need to make operations mutable.

#### Making Operation's forward and backward passes mutable ####

In order for us to allow implementations of operations to store state in their forward pass for use during their backward pass, we will need to change the trait implementation for our BasicOperation and ParameterOperation traits such that they look as follows:

```rust
pub trait BasicOperation<T>: BasicOperationClone<T> {
    fn calculate_output(&mut self, input: &Array<T, Ix2>) -> Array<T, Ix2>;
    fn calculate_input_gradient(
        &mut self,
        output_gradient: &Array<T, Ix2>,
        input: &Array<T, Ix2>,
    ) -> Array<T, Ix2>;
}

pub trait ParameterOperation<T>: ParameterOperationClone<T> {
    fn calculate_output(&mut self, input: &Array<T, Ix2>, parameter: &Array<T, Ix2>) -> Array<T, Ix2>;
    fn calculate_input_gradient(
        &mut self,
        output_gradient: &Array<T, Ix2>,
        input: &Array<T, Ix2>,
        parameter: &Array<T, Ix2>,
    ) -> Array<T, Ix2>;
    fn calculate_parameter_gradient(
        &mut self,
        output_gradient: &Array<T, Ix2>,
        input: &Array<T, Ix2>,
        parameter: &Array<T, Ix2>,
    ) -> Array<T, Ix2>;
}
```

That is, we simply need to change **&self** to **&mut self** in the calculation functions however this has the knock on effect then of requiring that all implementations are updated to the new signature (changing a trait is a breaking change in Rust).

I won't show that here, but again it's just a case of changing all the &self's to &mut self.

This has the unfortunate knock on effect of requiring that the Operation struct is also updated, this is because it will be calling these methods, which previously were able to be called on an immutable reference. Now, however it will need a mutable object to be able to call them, so we need to update Operation also.

Again, this is a simple enough change to not show here.

#### Identifying "inference" mode ####

Up until now, when we go through the forward pass, we have no idea if we're in training mode or inference mode. However for the dropout operation we will be required to know if we're in training or inference mode.

This is because, as mentioned previously when we're training the network, we want fully fledged dropout of neurons, however when we're only in inference mode we don't actually want to do any changes to the network, but still need to simulate the dropout.

To do this, we will firstly create an enum indicating the mode that we're in. We could use a boolean for this as there's only two states, however using an enumeration is more typesafe and more readable at the call site (it's better to see Mode.Inference instead of just "true").

```rust
#[derive(Clone, Copy, PartialEq)]
pub enum Mode {
    Training,
    Inference,
}
```

We will need to take the correct mode into the Network.forward method, since we only require the forward pass to be aware of what mode we're operating in. We have three "entry point" functions when interacting with the Network. These are:

1. **forward** - Called on a fully trained network to produce predictions. Should always run in inference mode
2. **train** - Called when we are training the network (performs a forward and backward pass). Always runs in training mode
3. **forward_loss** - Called both by training and also by examples on a trained network to see the loss values. Can run in both modes

Therefore we will move the contents into a helper function (inner_forward) which will take a Mode and can be called by both forward_loss and forward, passing the appropriate mode. We change forward to call this with a fixed mode of Inference, and change train to call it with a fixed mode of Training. forward_loss changes to take a Mode and pass it along. These 4 relevant functions now look as follows:

```rust
pub fn train(&mut self, batch: Array<T, Ix2>, targets: Array<T, Ix2>) -> T {
    let (_, loss) = self.forward_loss(batch, targets, Mode::Training);
    self.backward();
    loss
}

pub fn forward(&mut self, batch: Array<T, Ix2>) -> Array<T, Ix2> {
    self.inner_forward(batch, Mode::Inference)
}

pub fn forward_loss(
    &mut self,
    batch: Array<T, Ix2>,
    targets: Array<T, Ix2>,
    mode: Mode,
) -> (Array<T, Ix2>, T) {
    let predictions = self.inner_forward(batch, mode);
    let loss = self.loss.forward(predictions.clone(), targets);
    (predictions, loss)
}

fn inner_forward(&mut self, batch: Array<T, Ix2>, mode: Mode) -> Array<T, Ix2> {
    self.layers
        .iter_mut()
        .fold(batch, |state, layer| layer.forward(&state))
}
```

Next we will need to pass this along to the forward method of the **Layer** type inside inner_forward. Updating the code here is simple enough, we simply pass the mode into the layer.forward call.

However as you might have guessed, this doesn't compile!. Layer::forward is not expecting a second parameter, so we will need to make sure we add one:

```rust
// Layer::forward
pub fn forward(&mut self, input: &Array<T, Ix2>, mode: Mode) -> Array<T, Ix2> {
    // rest of the code is the same here
}
```

We're one level deeper now, we need to thread this inference mode down to the next lowest level which is into the individual operations themselves, which we can do inside the Layer::forward method when we're going over the operations. Adding this gives us a compile error once more, due to Operation::forward not expecting this parameter. So we add it:

```rust
// Operation::forward
pub fn forward(&mut self, input: &Array<T, Ix2>, mode: Mode) -> Array<T, Ix2> {
    // rest of code stays the same
}
```

Finally, we need to thread this mode into the **calculate_output** function on the BasicOperation or ParameterOperation trait object. However, you guessed it!, we need to update those traits to take the mode as a parameter into the calculate_output function:

```rust
// BasicOperation
fn calculate_output(&mut self, input: &Array<T, Ix2>, mode: Mode) -> Array<T, Ix2>;

// ParameterOperation
fn calculate_output(
    &mut self,
    input: &Array<T, Ix2>,
    parameter: &Array<T, Ix2>,
    mode: Mode
) -> Array<T, Ix2>;
```

As a result, we need to update all implementors to satisfy the new signature, but I won't show that here.

With that, we're finally done!, we are now able to specify the operating mode to the Network and it's correctly passed down all the way to the individual operations. We can finally implement the Dropout operation!

#### The dropout operation ####

We'll implement the dropout operation as a BasicOperation that we will tack onto the end of the appropriate layers, and with the above changes should be pretty easy to implement. We will need to store some internal state however. Namely, we need to record the generated random mask from the forward pass that we use to drop out some of the neurons so that we can drop the associated gradients on the backward pass (the gradient associated with an input we nullify shouldn't be passed back in the network).

We will also need to specify the proportion of the neurons that are **kept** which is the multiplier we use in inference mode, but also lets us generate the random mask in training mode.

The struct definition itself is pretty simple:

```rust
#[derive(Clone)]
pub struct Dropout<T> {
    mask: Array<T, Ix2>,
    keep_probability: T,
    seed: Option<u64>
}
```

Note that we also include the ability to specify a seed for the RNG to generate the mask. This is required for testing purposes as this is the only operation with a random element so we need to make sure we can handle a seed being passed.

As with the other operations, we will want to work with **boxed** versions of this type so as to be able to use it in the list of operations setup by our layer setup. Therefore we add a new_boxed method to produce a new Dropout instance. We will also add a new_boxed_with_seed function that specifies a seed for RNG also.

```rust
impl<T: Default> Dropout<T> {
    pub fn new_boxed(keep_probability: T) -> Box<Self> {
        Box::new(Self {
            mask: Default::default(),
            keep_probability,
            seed: None,
        })
    }

    pub fn new_boxed_with_seed(keep_probability: T, seed: u64) -> Box<Self> {
        Box::new(Self {
            mask: Default::default(),
            keep_probability,
            seed: Some(seed),
        })
    }
}
```

Note that we do require Default on elements of type T in order to use default on Array, even if we're creating an array of size 0, just because there seems to be no "empty" constructor to construct a new, empty array and Default is only implemented for Array in the case that the element type is also Default.

Now we just need to implement the BasicOperation trait for our Dropout type.

As far as trait bounds go, we will need the signature to look as follows:

```rust
impl<
        T: 'static + Clone + Mul<Output = T> + One + PartialOrd + SampleUniform + ScalarOperand + Zero,
    > BasicOperation<T> for Dropout<T>
where
    Standard: Distribution<T>,
{
    // implementation follows
}
```

The explanations for these are below:

1. **'static** - This is required because to be able to be clonable by the BasicOperationClone supertrait we need the elements to be of type T. The other operations so far have been empty, so the lifetime of T doesn't matter as we never store any, and the operation type is 'static. However for this one we store a mask of T's and so we need to explicitly say T has to be 'static for our type overall to be 'static.
2. **Clone** - This is required also for the cloning behaviour, we need T to be clonable so we can be clonable.
3. **Mul<Output=T>** - This is required because we must multiply the elements of the input by our keep probability which is a T, or by our mask which is an Array of T.
4. **One** - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what one of a T is.
5. **PartialOrd** - Required because we generate the dropout probabilities randomly, and need to determine whether a given value is kept or dropped. We do this by doing a comparison of the random value against our keep_probability.
6. **SampleUniform** - Required because we're using a random number generator to generate random samples within the range 0 to 1, and in a generic context, to do this we need to only use types that can be sampled in this way.
7. **ScalarOperand** - Required because we multiply the input by the keep probability if we're in inference mode. This allows the operation to be broadcast to the elements of the array correctly.
8. **Zero** - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what zero of a T is.
9. **Standard: Distribution\<T>** - This one is a little "backward" in that we're stating that the Standard trait from rand supports being a distribution over our type T.

For the calculate_output function, we firstly need to check whether we're in inference mode or not. If we are then we will simply multiply the elements of the input array by our keep probability to simulate the magnitude dropoff, but without actually dropping any neurons out. We will make a mask of all 1's just in case the backward pass is called (it shouldn't be in inference mode). This looks like follows:

```rust
fn calculate_output(&mut self, input: &Array<T, Ix2>, mode: Mode) -> Array<T, Ix2> {
    if mode == Mode::Inference {
        self.mask = Array::ones(input.raw_dim());
        input * self.keep_probability.clone()
    } else {
        // training mode implementation goes here
    }
}
```

Training mode is a little bit more complex!

Firstly we need to create an RNG either truly random or with the seed if we have one. We can do this in the same way as we do in the DenseLayerSetup type:

```rust
let mut random = match self.seed {
    Some(seed) => StdRng::seed_from_u64(seed),
    None => StdRng::from_rng(thread_rng()).unwrap(),
};
```

What we then need to do is calculate how many **total** elements we need to generate. This is easy enough - it's the number of rows multiplied by the number of columns:

```rust
let number_required = input.ncols() * input.nrows();
```

To generate the mask, we know that we will be generating a sequence of T's as a flat vector, and need to reshape it into a 2-D array of the correct size. This is pretty easy with the Array::from_shape_vec method we've been using:

```rust
self.mask = Array::from_shape_vec(
    (input.nrows(), input.ncols()),
    ???
).unwrap();
```

So we just have to fill in the ???'s which will produce the vector of randomly selected 1's and 0's.

To do this we can firstly create an iterator over the range (0..number_required) and then map that iterator onto one that produces T's. The map function we will pass doesn't actually use the index, so we replace that with an _.

The first thing we do is to generate a random T within the range 0 to 1. Since we're in a generic context, we don't know what 0 or 1 is in the context of a T, so we need to rely on the fact we've constrained T to implement the Zero and One traits so we can ask for the appropriate representations.

The actual generation can be done by the gen_range function provided by rand:

```rust
let elem: T = random.gen_range(T::zero()..=T::one());
```

Next we check if the generated element is less or equal to our keep probability. If it is, then we return a **1**. If however the generated element is above the keep probability then it's in the dropout range and should be emitted as a **0**. This snippet looks as follows:

```rust
    if elem <= self.keep_probability {
        T::one()
    } else {
        T::zero()
    }
```

Finally we can take that iterator that's now emitting random T::zero()'s and T::one()'s, and collect it into a vector for reshaping.

After we've got the mask, we just multiply the input by it to get the output:

```rust
input * self.mask.clone()
```

In its entirety, the calculate_output function looks as follows:

```rust
fn calculate_output(&mut self, input: &Array<T, Ix2>, mode: Mode) -> Array<T, Ix2> {
    if mode == Mode::Inference {
        self.mask = Array::ones(input.raw_dim());
        input * self.keep_probability.clone()
    } else {
        let mut random = match self.seed {
            Some(seed) => StdRng::seed_from_u64(seed),
            None => StdRng::from_rng(thread_rng()).unwrap(),
        };
        let number_required = input.ncols() * input.nrows();
        self.mask = Array::from_shape_vec(
            (input.nrows(), input.ncols()),
            (0..number_required)
                .map(|_| {
                    let elem: T = random.gen_range(T::zero()..=T::one());
                    if elem <= self.keep_probability {
                        T::one()
                    } else {
                        T::zero()
                    }
                })
                .collect(),
        )
        .unwrap();
        input * self.mask.clone()
    }
}
```

By contrast however, the calculate_input_gradient function is incredibly simple!. The only job for that function is to drop gradients associated with the elements that were dropped in the forward pass. We stored the mask that was generated so this is easy, and in fact I'll paste the code in its entirety here:

```rust
fn calculate_input_gradient(
    &mut self,
    output_gradient: &Array<T, Ix2>,
    _input: &Array<T, Ix2>,
) -> Array<T, Ix2> {
    output_gradient * self.mask.clone()
}
```

And with that, our Dropout operation is done!

---
# LayerSetupAdapter #

Now we need to provide a way to ensure that the Dropout operator can be opted into when creating the layers. The best way to do this is to write a wrapper around **any** implementation of a LayerSetup\<T> and ensure that we also implement the LayerSetup\<T>.

When layer_setup is called we can call the wrapped one to get the initial list of operations for the layer, and then we can tack a Dropout onto the end. This is called the Composite pattern and is quite pervasive in Rust because we don't have inheritance, instead the user can choose their functionality and combine it like this.

That way we don't need to duplicate the dropout functionality if a different LayerSetup other than DenseLayerSetup is created later.

The struct will be generic over two types:

1. **T** - The type of the underlying elements in the array
2. **U** - The type of the concrete instance of LayerSetup that we're actually decorating

It also needs to hold onto the keep_probability (also a T) for later during setup. The definition then looks as follows:

```rust
#[derive(Clone)]
pub struct DropoutLayerAdapter<T, U> {
    layer_setup: U,
    keep_probability: T,
}
```

We also need a new_boxed constructor function like we have for our other types, but this isn't that noteworthy so will show it here in it's entirety:

```rust
impl<T, U> DropoutLayerAdapter<T, U> {
    pub fn new_boxed(layer_setup: U, keep_probability: T) -> Box<Self> {
        Box::new(Self {
            layer_setup,
            keep_probability,
        })
    }
}
```

Finally, we need to actually implement the LayerSetup\<T> trait for our adapter so that it can stand in for the decorated type in the network. The trait bounds here seem funky:

```rust
impl<
        T: 'static + Clone + Default + One + PartialOrd + SampleUniform + ScalarOperand + Zero,
        U: Clone + 'static + LayerSetup<T>,
    > LayerSetup<T> for DropoutLayerAdapter<T, U>
where
    Standard: Distribution<T>
```

However, we don't need any of this functionality to implement the trait, these are simply bounds "inherited" from the various types in use that we're decorating (specifically LayerSetup, and Dropout).

The implementation is simple also, as we just need to:

1. Call the wrapped LayerSetup to get a list of Operations
2. Push our Dropout operation onto the end of the list
3. Return the modified list

In code, this equates to:

```rust
let mut operations = self.layer_setup.layer_setup(num_neurons, input, seed);
operations.push(Operation::new_basic(Dropout::new_boxed(
    self.keep_probability.clone(),
)));
operations
```

And that's it!

The caller can now create a DenseLayerSetup (or any LayerSetup) either without dropout as before:

```rust
DenseLayerSetup::new_boxed(Tanh::new_boxed())
```

Or with dropout, by wrapping it:

```rust
DropoutLayerAdapter::new_boxed(DenseLayerSetup::new(Tanh::new_boxed()), KEEP_PROBABILITY)
```

---
## Testing #

Finally we can create an example to test this new dropout functionality. We'll take the latest example which included:

1. Softmax cross entropy loss function
2. Momentum
3. Exponential learning rate decay

And we'll add dropout with a keep probability of 65% to the hidden layer (example is just called "dropout")....

```
Accuracy (training): 91.414%
Accuracy (testing): 91.522%
```

What happened!?

Well, we're dropping out neurons which will make it less likely to overfit, but will also slow down, or set back the training. Recall that dropout lets us increase the **capacity** of the network. Let's go ahead and do that by adding another hidden layer with twice the number of neurons:

```
Accuracy (training): 95.372%
Accuracy (testing): 95.322%
```

This is almost the same, but a little lower than we had previously. However we are able to have multiple layers now to model more complex relationships which dropout will cope with. Tweaking hyperparameters some more will likely result in gains.

As a sanity test, let's remove the dropout with this new layer:

```
Accuracy (training): 87.46199999999999%
Accuracy (testing): 87.434%
```

So we can see that indeed the dropout operator is letting us expand the layers and capacity of the network while preventing degraded accuracy.