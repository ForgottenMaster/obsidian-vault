So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.

This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.

In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).

However, in real life, objects don't suddenly change direction on an instant and instead if the velocity (gradient) stops or becomes 0, the object will slow down over time before coming to a stop.

We can simulate this *momentum* in our neural network optimisation also.

The rationale is that when we reach a trough in the graph, and the gradient is 0, the weights will still be updated for a time (although the rate will slow on successive iterations). This preservation of momentum can allow the weights to escape a false, or local minima.

---
# Formula #

What this means in practice is that the amount we use to update the weight of each parameter (the velocity) isn't just a function of the gradient *at that point*, and the learning rate but instead is a weighted average of the velocities at previous timesteps with the weights decaying exponentially.

The formula to calculate this sum is pretty simple and is defined as:

$$ update = \nabla_t + (\mu \times \nabla_{t-1}) + (\mu^2 \times \nabla_{t-2}) + ... $$

Where $\mu$ is the momentum parameter, or, how much the velocity from the previous timestep is degraded.

For example if the momentum parameter is 0.9, then in a given time step, we multiply the velocity from the previous timestep by 0.9, or 90%, the one from two timesteps ago by $0.9^2$ (0.81), the one from three time steps ago by $0.9^3$ (0.729), and so on. We then add them all up to get the total velocity for the current time step.

However in practice, we won't compute the entire history each update, as we'll soon see.

---
# Algorithm #

In order to implement the momentum, we can't be computing the entire historical sum every update, so we should find a better way to calculate what the velocity should be.

As it turns out, we can just do the following steps to update the velocity with momentum, and the current gradient:

1. Multiply the velocity by the momentum parameter (0.9 in the above example)
2. Add the current gradient in

As an example, this is what the stored velocity would represent at each timestep, starting at t=1:

1. $\nabla_1$
2. $\nabla_2 + (\mu \times \nabla_1)$
3. $\nabla_3 + (\mu \times \nabla_2) + (\mu^2 \times \nabla_1)$

And so on, all with us only needing to store a single quantity and not a history of previous quantities.

---
# Implementation #

To implement this, we will be create a new implementer of the **Optimiser** trait, so far we have only had the basic Stochastic Gradient Descent (SGD) optimiser.

We will still be following the same basic algorithm of SGD and updating the weight with a proportion of the gradient, however we will want to incorporate momentum into the calculation of velocity.

We call this new optimiser **SGDMomentum**

#### SDGMomentum ####
The structure itself is a bit more than with the regular SGD optimiser. The regular one only had a learning rate, which was the proportion of the gradient to update the weights with.

This new optimiser needs two additional pieces of data stored to retain state between update steps:

1. **velocities** - This will be a Vec of Arrays with the elements in the **same order** as they appear in the iterator returned by the parameters_and_gradients() function on Network. Thus, the velocity at index i will be the velocity for the parameter/weight at index i.
2. **momentum** - This is a simple parameter like learning rate which represents the multiplier we will update the current velocity with before updating it with the current gradient. Represents the degredation of velocity when no forces are acting on it (slowing down).

One thing to note is that we can only calculate the velocities when there is a valid iterator of parameter weights and gradients from the network, and this is only valid after one forward and backward pass have been performed. 

This means that when we create the optimiser the vector will be uninitialised and we will need to initialise it the first time we're optimising. We will therefore use Option to represent the None state initially, and it will be replaced with Some once the velocities are initialised.

The whole definition of our structure looks as follows

```rust
pub struct SGDMomentum<T> {
    velocities: Option<Vec<Array<T, Ix2>>>,
    learning_rate: T,
    momentum: T,
}
```

#### Construction ####

In order to construct a new instance of the Optimiser we **require** both a learning rate, and a momentum parameter. The velocities as mentioned previously are initialised to None by default until the first step is performed.

The "new" function we add to handle the creation of a new instance looks as follows

```rust
impl<T> SGDMomentum<T> {
    pub fn new(learning_rate: T, momentum: T) -> Self {
        Self {
            velocities: None,
            learning_rate,
            momentum,
        }
    }
}
```

#### Optimiser\<T> ####

Now we just need to implement the Optimiser\<T> trait for SGDMomentum\<T>, which requires only a single function, **step**. Step operates on a mutable reference to self, to allow us to retain state between calls (which is ideal because that's precisely what we're doing with velocities), and it takes a mutable reference to the Network to optimise.

The very first thing we will want to do is to check if the velocities list has been initialised (is some). If it's still None then this is the first time we've had the step function called, and we need to initialise one velocities array for each parameter weight entry, and in the same order.

Each velocity Array will have the same shape as the associated network parameter weight, since we will be performing an elementwise subtraction when we update the weights. The elements will start at all zeroes initially as the velocity is "at rest".

In order to check if we need to initialise the velocities, we can use the following function provided by Option

```rust
if self.velocities.is_none() {
    // initialisation code
}
```

What we want to do to create the Vec of velocities is take the iterator provided by the network that iterates over all the parameter weights in the network (provided by the parameters_and_gradients function), and for each element of that iterator, we want to produce an Array of the same shape that's all zeroes.

There is a method provided to us by the Iterator trait for this which is the map function. Finally, we can take that new iterator and use the **collect** function to allocate a Vec and collect the iterator into it.

The code for that looks as follows:

```rust
net.parameters_and_gradients()
    .map(|(param, _)| Array::zeros(param.raw_dim()))
    .collect()
```

Then we can do the actual update. For this we will once again want to iterate over the parameters and gradients in the network, but we want to do this in tandem with their current velocities.

Another provided method by the Iterator trait is **zip** which is called on an iterator, takes another iterator and produces a new iterator whose elements are a tuple of the elements from each.

Therefore each element in the zipped iterator will be a tuple containing the parameter/gradient, and the associated velocity for that parameter.

Finally we want to run "some code" for each element in the zipped iterator, which we can do with the for_each method.

This section of code then looks as follows:

```rust
net.parameters_and_gradients()
    .zip(self.velocities.as_mut().unwrap().iter_mut())
    .for_each(|((parameter, gradient), velocity)| {
        // do stuff here
    });
```

The code we put inside the callback to for_each will be the code to update **one** parameter, given its velocity.

We can do this elementwise by zipping the weight, gradient, and velocity arrays together which will let us iterate over their elements in an arbitrary order, but in lockstep with one another.

ndarray gives us a macro to perform this iteration, called **azip** (probably for Array Zip).

The way this macro is called is you provide a set of specifiers and arrays from which to draw the values for each specifier, in a syntax like a parameter list. The code to run for each set of elements goes in curly braces. For example, the following snippet:

```rust
azip!((parameter in parameter, gradient in gradient, velocity in velocity) {
    // do stuff here
});
```

will iterate over the elements of the parameter, gradient, and velocity arrays in lockstep with each other, and for each element will bind the appropriate element of parameter to the binding "parameter", the appropriate element of gradient to the binding "gradient", and velocity to "velocity". This is a feature in Rust called **shadowing** and is useful because it means that inside the function, we can't accidentally use them as arrays and are limited to only operating on the individual elements provided for that iteration.

Inside the function body, to update a single element we want to do the following sequence of steps:

1. Multiply the current velocity by momentum
2. Add the correct proportion of the current gradient to the velocity (using learning rate)
3. Update the parameter weight by the new velocity

We can use standard arithmetic operations for these, however we need to explicitly clone the various parameters since we're in a generic context and only require Clone, not Copy as a trait bound. The resulting function body is then:

```rust
*velocity = (*velocity).clone() * momentum.clone() + (*gradient).clone() * learning_rate.clone();
*parameter -= (*velocity).clone();
```

Note the use of the dereference (\*) operator, this is because we're iterating over references to arrays, meaning the elements we're iterating over are also references. We need to dereference them whenever we want to access the value (like to update the parameter, or velocity).

In order to support these operations, the following trait bounds are required on the element type T

1. **Clone** - Required because as mentioned before, we need to reuse things like the learning rate and momentum, and also arithmetic operators take operands by value (which we can't do in generics out of a reference, can't move from a reference)
2. **Mul<Output=T>** - This is required because we're multiplying a T by a T to produce a new T when we're updating the velocity
3. **SubAssign** - Used when updating the parameter by velocity, we use the -= operator
4. **Zero** - Required for initialising the velocities on the first update, initialised to all zeroes which requires we know what "zero" of a "T" means

The code then in its entirety looks as follows:

```rust
impl<T: Clone + Mul<Output = T> + SubAssign + Zero> Optimiser<T> for SGDMomentum<T> {
    fn step(&mut self, net: &mut Network<T>) {
        // if this is the first step/iteration, we should set up the velocities
        // at zeroes, but matching the dimensions of the parameters.
        if self.velocities.is_none() {
            self.velocities = Some(
                net.parameters_and_gradients()
                    .map(|(param, _)| Array::zeros(param.raw_dim()))
                    .collect(),
            );
        }

        // weight updating can be done with the parameters and gradients iterator in lockstep
        // with the velocities.
        let momentum = self.momentum.clone();
        let learning_rate = self.learning_rate.clone();
        net.parameters_and_gradients()
            .zip(self.velocities.as_mut().unwrap().iter_mut())
            .for_each(|((parameter, gradient), velocity)| {
                azip!((parameter in parameter, gradient in gradient, velocity in velocity) {
                    *velocity = (*velocity).clone() * momentum.clone() + (*gradient).clone() * learning_rate.clone();
                    *parameter -= (*velocity).clone();
                });
            });
    }
}
```

---
# Testing #

As you may recall from the previous post, the accuracy for the basic neural network was around **16%** which was the same as a simple linear regression. This seemingly is due to it falling into a local minima that it can't escape.

Momentum allows it to escape a local minima to get a higher accuracy, which is tested by replacing our standard SGD optimiser with the new SGDMomentum optimiser. For this test we will use a learning rate of 0.1, and a momentum of 0.9

After changing the code and running, against the same test as we had before, we get the following output:

```
Accuracy (training): 59.955999999999996%
Accuracy (testing): 60.126000000000005%
```

So we see that it's increased to "better than average". This is a decent jump from 16%!

For softmax, we were already at a pretty high accuracy, with the standard SGD optimiser scoring about **91%** accuracy.

However, momentum gives us gains even here, and when we run the same problem with the same network, and change to using the momentum based optimiser we get slightly better results again:

```
Accuracy (training): 93.47999999999999%
Accuracy (testing): 93.522%
```

So from these we can see that using a momentum-based optimiser is better than a basic SGD optimiser.

As a side note, using momentum doesn't improve linear regression at all. This is likely because linear regression can only simulate a straight line, so the best or most accurate it can get to it was already achieving, and using momentum to find that ideal state doesn't help.