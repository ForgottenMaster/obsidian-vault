There are a couple of things that were required to be tweaked as we go on with development of the extensions to the network API. This post will briefly explain the changes that were required before we move onto the extensions themselves.

---
# Changing the examples #

Eidetic at this point has examples that are showing the performance of a linear regression vs a basic neural network, both using the "mean squared error" loss function.

We are planning to add more examples to demonstrate the various techniques as we learn how we can potentially optimise the training of a network.

There are a couple of things we want to change at this point.

#### Changing the data set ####

The previous data set we were using for training was a "real world" data set of credit scores mapped to a category of whether those scores are "good" or "bad".

This works well, except that as we're learning we'd ideally like to avoid such a large data set (that data set had around 20 features) that will also bring in questions of data preparation and scaling that we're not prepared to answer yet.

The main issue being complexity, but also the fact that the individual features aren't necessarily related scale-wise.

We'd ideally like a simpler data set, still complex enough to allow us to demonstrate the training accuracy, but not so complex as to risk not being able to find a good fit when training.

Ideally we'd like a simple mathematical function. For this change, we will change all examples over to using a data set that is generated that has only 3 features and a target value.

The function we will be training to fit will be

$$ f(x, y, z) = (x < y) \\&\\& (x > z) $$

That is, for a given observation, 3 floating point numbers will be generated within some decent range of values and the target output value is either a 0 or a 1.

The target will be a 1 only if the first number generated is strictly **less than** the second AND is strictly **greater than** the third.

The code for this can be found [HERE](https://github.com/ForgottenMaster/eidetic/blob/main/examples/includes/data_wrangler.rs), but will be posted here in case of link breakage

```rust
use {
	ndarray::{Array, Axis, Ix2},
	rand::{
		Rng,
		SeedableRng,
		rngs::StdRng
	}
};

const TOTAL_OBSERVATIONS: usize = 100_000;
const MIN_VALUE: f64 = -100_000.0;
const MAX_VALUE: f64 = 100_000.0;

/// Gets data for testing with the neural network examples.
/// Data represents the following artiy-3 function we're trying to train
/// f(v1, v2, v3) = (v1 < v2) && (v1 > v3)
fn get_data(
    training_proportion: f64,
    seed: u64
) -> Data {
	// Generate the entire array of samples (including the column for the targets too).
	let mut random = StdRng::seed_from_u64(seed);
	let all_data = Array::from_shape_vec((TOTAL_OBSERVATIONS, 4), (0..TOTAL_OBSERVATIONS).flat_map(|_| {
		let v1 = random.gen_range(MIN_VALUE..=MAX_VALUE);
		let v2 = random.gen_range(MIN_VALUE..=MAX_VALUE);
		let v3 = random.gen_range(MIN_VALUE..=MAX_VALUE);
		let result = ((v1 < v2) && (v1 > v3)) as u8 as f64;
		std::iter::once(v1).chain(std::iter::once(v2)).chain(std::iter::once(v3)).chain(std::iter::once(result))
	}).collect::<Vec<_>>()).unwrap();
	
	// Split off the targets from the input data.
	let (input, targets) = all_data.view().split_at(Axis(1), 3);
	
	// determine the split point for the training/testing split.
	let split_index = (input.nrows() as f64 * training_proportion) as usize;
	let (training_input, testing_input) = input.split_at(Axis(0), split_index);
	let (training_targets, testing_targets) = targets.split_at(Axis(0), split_index);
	
	// we have the data to return now.
	Data {
		training_input: training_input.to_owned(),
		training_targets: training_targets.to_owned(),
		testing_input: testing_input.to_owned(),
		testing_targets: testing_targets.to_owned()
	}
}

struct Data {
	training_input: Array<f64, Ix2>,
	training_targets: Array<f64, Ix2>,
	testing_input: Array<f64, Ix2>,
	testing_targets: Array<f64, Ix2>
}
```

Essentially it iterates for TOTAL_OBSERVATIONS iterations, generating 3 random floats for each iteration, between MIN_VALUE and MAX_VALUE.

It then calculates the result, which as mentioned will either be a 0 or a 1.

This occurs inside of a flat_map call so it's expecting an iterator as a return value, so we use a series of "chain" calls to chain the 4 numbers together into an iterator.

The whole thing is collected into a Vec, which can then be turned into an Array for us to use.

The rest of the code is just slicing the array into training and testing, and separating the input data from the targets.

#### Changing the example output ####

The examples were previously printing out a message to show the final error/loss value after training for both of the data sets.

The problem here is that this loss value kind of doesn't tell us what scale we're using, and it's certainly possible to have a high error/loss value but still have an accurate trained network (as is the case when we look at the softmax cross entropy loss function).

What we actually care about is the percentage of predictions that the network makes that match the targets correctly.

Therefore we'll change the examples to output as follows

```rust
// Accuracy
let training_predictions = network
    .forward_loss(data.training_input, data.training_targets.clone())
    .0;
let training_correct_count = training_predictions
    .iter()
    .zip(data.training_targets.iter())
    .filter(|(prediction, target)| {
        let prediction = if **prediction < 0.5 { 0.0 } else { 1.0 };
        prediction == **target
    })
    .count();
let training_accuracy =
    (training_correct_count as f64 / data.training_targets.nrows() as f64) * 100.0;
println!("Accuracy (training): {training_accuracy}%");

let testing_predictions = network
    .forward_loss(data.testing_input, data.testing_targets.clone())
    .0;
let testing_correct_count = testing_predictions
    .iter()
    .zip(data.testing_targets.iter())
    .filter(|(prediction, target)| {
        let prediction = if **prediction < 0.5 { 0.0 } else { 1.0 };
        prediction == **target
    })
    .count();
let testing_accuracy =
    (testing_correct_count as f64 / data.testing_targets.nrows() as f64) * 100.0;
println!("Accuracy (testing): {testing_accuracy}%");
```

For both training and testing data sets, we run them through the trained network and take the predictions array.

Then we zip the predictions array with the target array (we know they both only have a single column/feature) and if the prediction is less than 0.5 consider it 0, else it's a 1 if above 0.5.

Then we just compare with the target value and filter those that match.

The count() function will give us the count of matching predictions and to work out the percentage we can divide it by the total number of targets and multiply by 100 to get the percentage to print out.

#### Conclusion ####

With those two changes to the data set used by the examples, we have a nice simple but predictable function that we know should be trainable, and we can therefore see how well the various training strategies work.

---
# Tanh activation function #

When we're training a neural network, the biggest factor in how successful we would be is how much we can use the gradient to update the weights on the neurons in the layers during the backpropagation pass.

However, we firstly are only updating the weights based on a **proportion** of the gradient (the learning rate), and secondly are limited by the possible gradients of the activation function we chose.

With the sigmoid activation function, the maximum gradient we can have is 0.25, however with the tanh function we'll implement, we are able to get gradients of 1.

These steeper gradients allow us to train the network much faster, as is evident by the following graph comparing the derivatives/gradients of the sigmoid function we were using with the tanh function we want to use:

![Derivative Comparison](Derivative%20Of%20Activation%20Function%20Comparison.png)

The tanh function is a non-linear function like sigmoid except it maps to a range of -1 to 1, rather than to a range of 0 to 1.

The additional advantage is that we can map to negative numbers and not just positive values. This is important as discovered since negative weights on neurons allow some neurons to "inhibit" the output and can subtract from the result.

With positive weights only, we can only create additive weights which results in a more narrow range of output values.

The graph of tanh is as follows, for visualisation

![Tanh Graph](Tanh%20Graph.png)

#### The formulas ####
The formula for tanh is actually pretty simple, it's related to the sigmoid function from before (also known as the logistic function) by being a moved and scaled version.

The formula for tanh is just as follows

$$ tanh(x) = \frac {e^x - e^{-x}} {e^x + e^{-x}} $$

As with the sigmoid function, the tanh function also has a very nice and easy derivative calculation, defined in terms of itself

$$ tanh'(x) = 1 - tanh(x)^2 $$

#### The code ####

Now we need to code this up to be able to use it in a network as an activation function. As with all of our other operations in the library, it is just an empty struct which will behave as a vtable for our function calls.

```rust
#[derive(Clone)]
pub struct Tanh;
```

Nextly we need to be able to construct a boxed version for use in a layer. We will follow the convention set out by the API and call this function "new_boxed" and it will look as follows

```rust
impl Tanh {
    pub fn new_boxed() -> Box<Self> {
        Box::new(Self)
    }
}
```

Finally we will need to implement the **BasicOperation\<T>** trait. For this Operation, we need access to functions provided by the num_traits::Float trait, so we constrain it as such:

```rust
impl<T: Float> BasicOperation<T> for Tanh {
    // code
}
```

For the two required functions, these are easy enough to implement. For applying the Tanh function, we actually have this as a pre-made function on the Float trait.

Therefore when we implement calculate_output, we are just mapping the input array into a new one, where tanh is called on each element

```rust
fn calculate_output(&self, input: &Array<T, Ix2>) -> Array<T, Ix2> {
    input.mapv(|elem| elem.tanh())
}
```

Calculating the gradient as mentioned before, is defined in terms of the square of the tan (subtracted from 1). We must remember that that only gives us our partial derivative, and we still need to multiply our partial gradient with the output gradient passed back from the next layer.

Therefore the full code of this function looks thusly

```rust
fn calculate_input_gradient(
    &self,
    output_gradient: &Array<T, Ix2>,
    input: &Array<T, Ix2>,
) -> Array<T, Ix2> {
    output_gradient * input.mapv(|elem| T::one() - elem.tanh() * elem.tanh())
}
```

---
# Weight initialization #

Our current weight initialization is initializing to random weights between 0 and 1, that is they are always positive weights and as mentioned before don't have the ability to "inhibit" the result.

In order to get a decent range for generating our random weights at initialisation time, we can use a formula called **Xavier weight initialisation**.

The formula is as follows

$$ \pm \frac {\sqrt 6} {\sqrt {n_i + n_{i+1}}} $$

In this formula, $n_i$ is the number of input neurons (number of features/columns in the input data), also called **fan-in**, and $n_{i+1}$ is the number of neurons output by this layer, also called **fan-out**

For coding this up, we simply need to go into the **DenseLayerSetup** layer which is responsible for initializing weights, and ensure that this formula is used to determine the minimum and maximum values for weight generation.

The relevant code can be found in dense_layer_setup.rs and looks as follows

```rust
let xavier_delta = T::from(6_u8).unwrap().sqrt()
    / (T::from(input.ncols()).unwrap() + T::from(num_neurons).unwrap()).sqrt(); // see Xavier initialization
let weights = repeat_with(|| r.gen_range::<T, _>(-xavier_delta..=xavier_delta))
    .take(weight_count)
    .collect();
```

Note that because we're inside of a generic context, in order to get the square root of 6, we first need to turn 6 into a "T". We can do this with the "from" function provided by the num_traits::Float trait.

To get the number of input neurons, we can just get the number of columns in the input data. The number of output neurons is provided to us in the function parameters.

To generate the weights, we can just use the **gen_range** method on the random number generator and specify +/- this calculated Xavier value.