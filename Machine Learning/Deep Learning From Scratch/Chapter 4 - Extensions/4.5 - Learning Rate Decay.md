As we've covered a couple of different optimisation strategies (stochastic gradient descent with and without momentum), it's become clear that the learning rate **hyperparameter** is one of, if not *the* most important hyperparameters in deep learning.

A hyperparameter in deep learning is a tweakable value that needs to be tuned based on the problem at hand. These include the learning rate, momentum, epoch count, batch size, etc. and half of the success for training a network is finding the correct hyperparameter values.

If we refer back to the diagram seen previously, which can be seen below

![[Diagram Of Training.png]]

We can see that with small values for the learning rate (the blue arrows), the steps we take are small but we run the risk of falling into a local minimum. Conversely, if the learning rate is too large (the red arrows), we can see that we "bounce around" and might skip over the true minimum.

What we would like is a happy balance of the two approaches, in fact we'd like to start the training with a larger learning rate to broadly find the region of the minimum, but as we progress through training, we'd like to take smaller and smaller steps so as to hone in on the true minimum and be less likely to skip over it.

We can achieve this by **decaying** the learning rate hyperparameter as we progress through the epochs.

---
# Refactoring #

In order to allow for decaying the learning rate over the process of training the network, we will need to (slightly) refactor the existing code such that we can later drop in a decaying learning rate if needed.

#### Optimiser trait ####
The first step will be to add a couple of additional methods to the Optimiser trait, these will be:

1. **init(epochs)** - Called at the very beginning of the training process with the number of epochs we plan to run through. This allows a chance to set up data at the beginning of the process that might rely on that total epoch count (such as a percentage progress, or something else).
2. **end_epoch()** - Called at the end of each epoch EXCEPT for the very last one, and gives the Optimiser a chance to update state at the end of the epoch before it ends up getting stepped again (allowing for example learning rate to decay).

After we add these, the Optimiser\<T> trait looks as follows:

```rust
pub trait Optimiser<T> {
    fn init(&mut self, epochs: u32);
    fn step(&mut self, net: &mut Network<T>);
    fn end_epoch(&mut self);
}
```

#### train function ####

I'll show how we implement this into our Optimiser implementations in a moment, but let's take a look at the changes required to the train function to invoke these new functions. Firstly at the beginning of the train function before entering the epoch loop, we call init:

```rust
let mut best_loss: Option<T> = None;
optimiser.init(epochs);
for e in 0..epochs {
    // rest of code follows
```

And at the end of the loop, we call the end_epoch function. However, we only do this if the epoch number is not the very last one. If it's the very last one then there's no point updating the optimiser as it's not going to be invoked again. The code therefore looks as follows:

```rust
    if let Some(last_model) = last_model {
        let (_, test_loss) = network.forward_loss(batch_test.clone(), targets_test.clone());
        if best_loss.is_none() || test_loss.abs() < *best_loss.as_ref().unwrap() {
            best_loss = Some(test_loss.abs());
        } else {
            *network = last_model;
            break;
        }
    }

    if e < (epochs - 1) {
        optimiser.end_epoch();
    }
```

#### SGD ####

For the SGD optimiser, we will need to implement the new functionality required by the trait, however we know that we will be implementing the new functionality in the SGDMomentum type also in the exact same way. In order to prevent duplicated code, and in order to support easily picking which strategy we're using for handling the learning rate, we will delegate the handling of the learning rate to a new type.

We'll talk about that soon, but for now just be aware that instead of directly storing a T inside our SGD type, we will be storing a different type that will implement a particular interface.

For the struct definition, we don't actually need to change anything!. We only had one field (learning_rate), so we can simply rename this to indicate it should be a handler for our learning rate instead:

```rust
pub struct SGD<T> {
    learning_rate_handler: T,
}
```

We rename it also in the new and new_boxed functions:

```rust
impl<T> SGD<T> {
    pub fn new(learning_rate_handler: T) -> Self {
        Self {
            learning_rate_handler,
        }
    }

    pub fn new_boxed(learning_rate_handler: T) -> Box<Self> {
        Box::new(Self::new(learning_rate_handler))
    }
}
```

For the trait implementation, this is where we will need to tell the compiler about this new trait type that we have that will represent our handler, or wrapper that is responsible for storing and updating the learning rate hyperparameter.

To do this, we need to introduce a second generic parameter. We place a trait bound on that generic parameter to indicate that it must implement LearningRateHandler, and the type that it's handling/wrapping around must be the same type as the elements in the Network.

After adding this, it looks like follows:

```rust
impl<T: LearningRateHandler<U>, U: LinalgScalar + ScalarOperand> Optimiser<U> for SGD<T> {
    // implementation here
}
```

In this, we're saying:

1. **T** is the type that's stored inside the SGD and is handling the learning rate hyperparameter. Our trait bound indicates that this type is implementing LearningRateHandler and the type that it's handling is our element type, U
2. **U** is the type of the elements inside the neural network we're optimising, and contains all the same bounds as it previously did

As for the implementations of the new function, we will delegate to the handler type. This will allow us to have a central handler type with the correct degregation logic, and have that shared between our SGD and SGDMomentum (and any other Optimiser implementations we might write).

The new function implementations then just look as follows:

```rust
fn init(&mut self, epochs: u32) {
    self.learning_rate_handler.init(epochs);
}

fn end_epoch(&mut self) {
    self.learning_rate_handler.end_epoch();
}
```

The only other change is that since we aren't storing the learning rate directly now, we must access it instead through the handler object when we're optimising the network parameters:

```rust
*param = &*param - (gradient * *self.learning_rate_handler.learning_rate());
```

#### SGDMomentum ####

The same changes are required here basically as we made to the SGD. That is:

1. We need to change learning_rate inside the type to be learning_rate_handler
2. This type needs to be independant of the element type (which we use for momentum, and as the element type for storing velocities)
3. We need to implement the new functions from the Optimiser trait, and delegate their behaviour to our LearningRateHandler implementation
4. We need to make sure we get the current learning rate by calling the learning_rate() method on our LearningRateHandler implementor

#### LearningRateHandler ####

This is our new trait that we are using to define the behaviour and capabilities of whichever type is wrapping the actual learning rate. This trait needs to have the following functionalities:

1. It needs to be able to provide the current learning rate to a caller
2. It needs to be able to be initialised at the beginning of training, with the number of epochs that will be run
3. It needs to be able to be updated at the end of an epoch

When coded up, this is pretty easy:

```rust
pub trait LearningRateHandler<T> {
    fn learning_rate(&self) -> &T;
    fn init(&mut self, epochs: u32);
    fn end_epoch(&mut self);
}
```

#### LearningRateFixed ####

The final part of refactoring the existing stuff to support the potential for decaying learning rates, while making sure the existing tests and examples continue to work, is a handler for our learning rate that behaves the same as we had our learning rate behaving before.

That is, a LearningRateHandler that is initialised with a learning rate, and never changes it.

For this struct, it will just contain the actual fixed learning rate inside of itself, and the "new" function is similarly uneventful:

```rust
pub struct LearningRateFixed<T> {
    learning_rate: T,
}

impl<T> LearningRateFixed<T> {
    pub fn new(learning_rate: T) -> Self {
        Self { learning_rate }
    }
}
```

We'll implement the LearningRateHandler trait for this type, but the only thing it does is to return a reference to the inner learning_rate when asked for it:

```rust
impl<T> LearningRateHandler<T> for LearningRateFixed<T> {
    fn learning_rate(&self) -> &T {
        &self.learning_rate
    }

    fn init(&mut self, _epochs: u32) {}

    fn end_epoch(&mut self) {}
}
```

Finally, we just need to make sure wherever we construct an SGD or an SGDMomentum instance, that instead of passing the learning rate, we're now passing an instance of a fixed handler. We just go through and change all the tests/examples to look something like:

```rust
let mut optim = SGDMomentum::new(LearningRateFixed::new(0.1), 0.9);
```

And with that, our refactor is done!

The next couple of sections of this post will be dedicated to the actually decaying handler types.

---
# Linear Decay #

The first of the two types of decay that we'll cover is a decay that has a fixed size step between epochs, that is, it always decays the current learning rate by a fixed amount each epoch.

The formula for this is

$$ \alpha_t = \alpha_{start} - (\alpha_{start} - \alpha_{end}) \times \frac t {epochs - 1} $$

But rather than computing quantities each frame, we will calculate the amount to be subtracted per epoch at the point where the handler is initialised. We then simply subtract that each time end_epoch is called.

#### Struct ####
For the structure itself, we will need to store several quantities which will be:

1. The defined starting learning rate. This is the starting rate that the current rate will be set to whenever the init function is called.
2. The defined ending learning rate. After the maximum number of epochs is called, the learning rate will end up being this value (or close to it due to rounding errors).
3. The current learning rate
4. The amount of learning rate to subtract each epoch

These are generic as with everything else, but the type is just defined as "T":

```rust
pub struct LearningRateLinearDecay<T> {
    starting_rate: T,
    ending_rate: T,
    current_rate: T,
    decay_per_epoch: T,
}
```

#### Constructor ####

In order to construct a new instance of this structure, the **required** fields are the starting and ending learning rates that must be passed in. The other two fields are used internally and calculated from these two.

We will however require the **Clone** trait bound on T, because we firstly need to initialise the current learning rate to the starting rate anyway, but also because we need to initialise our **decay_per_epoch** field to *something*. We don't mind what we set it to because it'll be recalculated when we initialise during training, but it must be set to something, so we'll just use the starting rate as a valid value.

The whole constructor function then looks as follows:

```rust
impl<T: Clone> LearningRateLinearDecay<T> {
    pub fn new(starting_rate: T, ending_rate: T) -> Self {
        Self {
            starting_rate: starting_rate.clone(),
            ending_rate,
            current_rate: starting_rate.clone(),
            decay_per_epoch: starting_rate, // this will get recalculated during training
        }
    }
}
```

#### Trait ####

Now we just need to implement the LearningRateHandler trait so that we can use this type as a decaying learning rate with one of our optimisers.

The init function will simply calculate the amount of decay per epoch. In order to do this, it will use the following formula:

$$ decay = (start - end) / (epochs - 1) $$

That is, it takes the difference between start and end (the **distance** to decay over), and the number of epochs it needs to decay over (the **time** it has to decay), and calculates the rate of decay (the **speed**).

The number of epochs is epochs **- 1** rather than epochs because we don't call the end_epoch function on the very last epoch so we need to make sure we reach the ending learning rate after we call end_epoch for the very last time (which is 1 iteration fewer than the total number of steps we're doing).

Coding that up, it will look as follows:

```rust
fn init(&mut self, epochs: u32) {
    let epochs: T = epochs.into();
    self.decay_per_epoch =
        (self.starting_rate.clone() - self.ending_rate.clone()) / (epochs - T::one());
}
```

Note that we need to clone starting and ending rate as the subtraction operator consumes its inputs. We also need to convert the number of epochs from a u32 into a T before dividing. We do this rather than requiring a bound on Div\<u32> (as in, dividing a T by u32), because it's more common to have a Div implementation that takes a T on the right hand side.

Returning the current learning_rate is a trivial operation:

```rust
fn learning_rate(&self) -> &T {
    &self.current_rate
}
```

Finally, when the end_epoch is called, we modify the current learning rate by simply subtracting the decay amount. Again though, we must clone it as the SubAssign trait consumes the right hand side input:

```rust
fn end_epoch(&mut self) {
    self.current_rate -= self.decay_per_epoch.clone();
}
```

We put all these implementations inside an impl block for the trait, making sure to place the appropriate trait bounds on T based on the operations we required inside the implementations.

The block then looks as follows:

```rust
impl<T: Clone + Div<Output = T> + From<u32> + One + Sub<Output = T> + SubAssign> LearningRateHandler<T>
    for LearningRateLinearDecay<T>
{
    // implementations here
}
```

---
# Exponential Decay #

Whereas linear decay applies a fixed decay to the learning rate each step, the exponential decay instead applies a **multiplier**.

The definition of the struct, and the new function actually is identical to the linear version. This is because we need the exact same parameters, it's just we're calculating the decay multiplier differently, and applying it to the learning rate differently too.

The struct definition and new function then look as follows

```rust
pub struct LearningRateExponentialDecay<T> {
    starting_rate: T,
    ending_rate: T,
    current_rate: T,
    decay_per_epoch: T,
}

impl<T: Clone> LearningRateExponentialDecay<T> {
    pub fn new(starting_rate: T, ending_rate: T) -> Self {
        Self {
            starting_rate: starting_rate.clone(),
            ending_rate,
            current_rate: starting_rate.clone(),
            decay_per_epoch: starting_rate,
        }
    }
}
```

The changes come in the implementation of the trait itself. The formula we use to calculate the decay multiplier is as follows:

$$ {\frac {end} {start}}^{\frac {1} {epochs - 1}} $$

Which is coded up as below in the init function

```rust
fn init(&mut self, epochs: u32) {
    let epochs: T = epochs.into();
    self.decay_per_epoch = (self.ending_rate / self.starting_rate).powf(T::one() / (epochs - T::one()));
}
```

Accessing the learning rate in the learning_rate method remains the same as the linear version. The only other change except from calculating the decay rate is actually applying it.

In the linear version we had used subtraction to subtract the decay amount from the learning rate at the end of an epoch, however in the exponential version we need to multiply it instead, so the end_epoch method looks as follows

```rust
fn end_epoch(&mut self) {
    self.current_rate *= self.decay_per_epoch;
}
```

---
# Testing #

In order to test these decay methods, we will create two new examples, based off the **softmax_cross_entropy_loss_with_momentum** example. These will be:

1. **softmax_cross_entropy_loss_with_linear_decay** - A version that applies a linear decay to the learning rate.
2. **softmax_cross_entropy_loss_with_exponential_decay** - A version that applies an exponential decay to the learning rate.

As a refresher, we'll re-run the base version (with no decay) and we can see that we get a decent accuracy:

```
Accuracy (training): 93.47999999999999%
Accuracy (testing): 93.522%
```

For the linear decay, we'll still start with a learning rate of 0.1, but we'll decay down to 0.05 over our epochs, and the results we can see are slightly better:

```
Accuracy (training): 94.582%
Accuracy (testing): 94.624%
```

Finally, we'll test the exponential decay with the same start and end learning rates, however we see a sudden drop in accuracy!. Using exponential decay results in a drop to 90% accuracy.

This is because the decay rate with exponential is **much** steeper, and so we are taking too small a step too quickly.

One thing this steep decay lets us do though is take larger steps at the beginning. Let's go ahead and change the starting learning rate to something slightly higher (0.2) and we see that the results are better again:

```
Accuracy (training): 95.506%
Accuracy (testing): 95.562%
```

95% aint bad!