At this point we've successfully coded a neural network library that should be extensible enough to create various architectures of **densely connected** layers. We've also implemented various techniques that can be used to potentially get better results from the network.

The next chapters will be about entirely new network architectures, namely **Convolutional Neural Networks** (CNNs) and **Recurrent Neural Networks** (RNNs), but now that we have enough pieces for training a standard feed forward network, I want to try out training on something more substantial and meaningful than just a simple mathematical formula or comparison operation such as we've seen so far.

This post will cover using the library we've made to train a feed forward network that can recognize handwritten numbers from small images. Specifically, using the MNIST data set.

---
# What is the MNIST data set? #

The details of the MNIST data set can be read on the [Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database) but basically it's a curated data set from several earlier image data sets, curated to be ideal for machine learning.

The images in the set are all handwritten digits, examples of the images are below for the digits 0-9:

![MNIST samples](MNIST%20Samples.png)

The actual files are located [here](http://yann.lecun.com/exdb/mnist/) and there are 4 files, grouped into testing and training data. The files beginning with **train** are the ones corresponding to the training data, and the ones beginning with **t10k** are the testing ones. Among these two pairs, the ones labelled with **images** are the actual input image data, and the ones labelled with **labels** are the correct answers in the same order as the images.

The images are each 28x28 pixels, and the labels are simply numbers from 0-9 indicating the target value. However, these files are in a custom binary file format so we will first have to be able to parse them.

---
# The IDX file format #

This is detailed nicely on the website but the IDX file format that the images and labels are in is a nice simple binary format for storing N-dimensional data with varying data types/element sizes.

The data format is as follows:

```
(0x0000) magic number (4 bytes)
(0x0004) size in dimension 0 (4 bytes)
(0x0008) size in dimension 1 (4 bytes)
(0x000c) size in dimension 1 (4 bytes)
...
(0x????) size in dimension N (4 bytes)
(0x????) data element 1 (varying)
(0x????) data element 2 (varying)
...
(0x????) data element N (varying)
```

That is, there's a fixed size magic number followed by the sizes of the dimensions, and then finally the data follows.

The magic number is a **4 byte** integer with the first 2 bytes always **0**.

The third byte encodes the data type of the elements in the data segment which can be:

- **0x08**: u8  (1 byte)
- **0x09**: i8  (1 byte)
- **0x0B**: i16 (2 bytes)
- **0x0C**: i32 (4 bytes)
- **0x0D**: f32 (4 bytes)
- **0x0E**: f64 (8 bytes)

The last byte tells us how many dimensions there are in the data (e.g. 1 for vector, 2 for matrix, etc.)

All integers are encoded as big endian in the byte stream (most significant bit first).

As for coordinates, the index of the last dimension in the data changes the fastest (for example if storing a 2 dimensional matrix then the first row is read, then the second row, and so on).

In the following sections in this post we'll implement a parser in Rust that can take a **byte slice** and produce a nicely parsed data structure that can then be used by our code to turn it into an ndarray::Array for processing for example.

---
# Data type #

Starting out with the most basic type which is an enumeration that indicates the **type** of the data in the file, from the set detailed above. Firstly we'll need to make some constants (not strictly necessary but always good to have something the compiler can verify, and collect them at the top of the file). These are simply encoded as u8's:

```rust
const U8: u8 = 0x08;
const I8: u8 = 0x09;
const I16: u8 = 0x0B;
const I32: u8 = 0x0C;
const F32: u8 = 0x0D;
const F64: u8 = 0x0E;
```

For the actual data type, we know we'll have the small well-defined set above and an enumeration type in Rust is perfect to represent this. We'll need the **Clone, Debug, and PartialEq** traits for later, so we will make sure to tell the compiler to automatically derive implementations for these:

```rust
#[derive(Clone, Debug, PartialEq)]
pub enum DataType {
    U8,
    I8,
    I16,
    I32,
    F32,
    F64,
}
```

As far as functionality goes, we will need to be able to do the following two things with DataTypes:

1. Ask an instance of DataType how many bytes it takes up, for when we read the data
2. Be able to convert a u8 into a DataType

For now we'll just assume that the u8 => DataType conversion is infallible and panic if a value is given that can't be converted. In a more robust library we should use TryFrom instead to allow better error handling. I don't mind this just terminating the program if the data is malformed.

For #1, we simply need to use a match with the value and return the appropriate number of bytes which will either be 1, 2, 4, or 8:

```rust
pub fn number_of_bytes(&self) -> u8 {
    match self {
        Self::U8 | Self::I8 => 1,
        Self::I16 => 2,
        Self::I32 | Self::F32 => 4,
        Self::F64 => 8,
    }
}
```

For #2, we can use a match also - we'll use the constants we defined for readability. We'll use the panic! macro with a message if the u8 we're converting from isn't a valid data type value:

```rust
impl From<u8> for DataType {
    fn from(byte: u8) -> Self {
        match byte {
            U8 => Self::U8,
            I8 => Self::I8,
            I16 => Self::I16,
            I32 => Self::I32,
            F32 => Self::F32,
            F64 => Self::F64,
            _ => panic!("Invalid byte given to be converted into a data type: {byte}"),
        }
    }
}
```

---
# Value #

We now have the ability to determine what the type of the data is, but no way to represent the **actual** data that has been read.

For this we'll use another enum type called Value which will hold the actual value as a rust primitive type. However, because there are a range of DataType variants, we likewise need to define the same variants here. The definition is as follows (again we need to derive the Debug and PartialEq types):

```rust
#[derive(Debug, PartialEq)]
pub enum Value {
    U8(u8),
    I8(i8),
    I16(i16),
    I32(i32),
    F32(f32),
    F64(f64),
}
```

The first conversion we require is a conversion from a byte sequence/slice into a Value instance however if we're just given a slice of bytes, how do we know how many we're supposed to read and interpret or what data type to interpret as?.

We can't is the short answer, we need another piece of information which is the **data type**. Conveniently, we've just added a way to represent the data type.

The conversion then, is not from a byte slice to Value, but from a **tuple** of byte slice **and** DataType.

Again, we'll assume an infallible conversion hence implementing From rather than TryFrom, but in a more robust library we'd want to handle failure nicely.

The conversion itself will simply match on the data type, then depending on the length of the data type we will want to ensure there's enough bytes to take, and then take those bytes and construct the appropriate data type from them, assuming a big endian ordering.

Rust has an associated method **from_be_bytes** for all primitive types, which will take the bytes and construct an instance of the appropriate type with them.

The one thing to be aware of is from_be_bytes takes a byte **array** however we have a byte **slice**. Helpfully though, there's a fallible conversion from slice to fixed array, so we can use try_into. However since we ourselves are not handling errors, we'll just unwrap this. The whole implementation then is as follows:

```rust
impl From<(&[u8], DataType)> for Value {
    fn from(tuple: (&[u8], DataType)) -> Self {
        let (data, data_type) = tuple;
        match data_type {
            DataType::U8 => {
                assert!(data.len() >= 1);
                Value::U8(data[0])
            }
            DataType::I8 => {
                assert!(data.len() >= 1);
                Value::I8(i8::from_be_bytes(data[0..1].try_into().unwrap()))
            }
            DataType::I16 => {
                assert!(data.len() >= 2);
                Value::I16(i16::from_be_bytes(data[0..2].try_into().unwrap()))
            }
            DataType::I32 => {
                assert!(data.len() >= 4);
                Value::I32(i32::from_be_bytes(data[0..4].try_into().unwrap()))
            }
            DataType::F32 => {
                assert!(data.len() >= 4);
                Value::F32(f32::from_be_bytes(data[0..4].try_into().unwrap()))
            }
            DataType::F64 => {
                assert!(data.len() >= 8);
                Value::F64(f64::from_be_bytes(data[0..8].try_into().unwrap()))
            }
        }
    }
}
```

As far as extracting out the contained value, we actually only end up needing the u8 variant since the MNIST data is only bytes.

Well therefore only implement for now the conversion into u8, but in a real package, we'd make sure there was a way to access the other variants too. Implementation is simply matching on variant and if it's U8 then we can pull out the value inside:

```rust
impl From<&Value> for u8 {
    fn from(value: &Value) -> Self {
        match value {
            Value::U8(value) => *value,
            _ => unimplemented!(),
        }
    }
}
```

---
# Magic number #

The first item in an IDX file is a "magic number" which indicates both the type of the data elements, and the number of dimensions.

The data type we have an enumeration for, and the number of dimensions is encoded in a single byte of the magic number, so we use a u8 for storage.

The definition then is:

```rust
#[derive(Debug)]
pub struct MagicNumber {
    data_type: DataType,
    num_dimensions: u8,
}
```

For conversions, we will need to:
1. Convert from a 4 byte array to a MagicNumber
2. Decompose a MagicNumber into its data type and number of dimensions

For #1, we need to implement the From<[u8; 4]> trait for MagicNumber, and the implementation will do the following:
1. Check that bytes 0 and 1 are 0
2. Convert byte at index 2 into a DataType
3. Construct and return the MagicNumber struct

This functionality is coded up as:

```rust
impl From<[u8; 4]> for MagicNumber {
    fn from(bytes: [u8; 4]) -> Self {
        assert_eq!(bytes[0], 0);
        assert_eq!(bytes[1], 0);
        let data_type = bytes[2].into();
        let num_dimensions = bytes[3];
        Self {
            data_type,
            num_dimensions,
        }
    }
}
```

Conversion #2 can be implemented also with a From trait, this time From\<MagicNumber>, and we implement it on the type (DataType, u8).

We just return our fields, so the implementation is simple enough:

```rust
impl From<MagicNumber> for (DataType, u8) {
    fn from(magic_number: MagicNumber) -> Self {
        (magic_number.data_type, magic_number.num_dimensions)
    }
}
```

---
# File contents #

Now we have the ability to parse the magic number containing the data type and number of dimensions, we can go ahead and parse the file contents as a whole. Once we've parsed the magic number from the beginning of the buffer, we need to:
1. Read the next N groups of 4 bytes. Each one represents an unsigned integer indicating the size of one of the dimensions.
2. After reading the dimension sizes, read the rest of the data into a buffer (as the appropriate Value type).

The structure we'll be using to store everything in will be called the FileContents struct and will just need to store a Vec of sizes (for the dimensions - a Vec because we don't know in advance how many dimensions there are), along with a Vec of Value type holding the data linearly.

The struct definition looks as follows:

```rust
#[derive(Debug, PartialEq)]
pub struct FileContents {
    sizes: Vec<u32>, // vector of dimension sizes
    data: Vec<Value>,
}
```

We make readonly accessors for these, pretty simple:

```rust
impl FileContents {
    pub fn sizes(&self) -> &[u32] {
        &self.sizes
    }

    pub fn data(&self) -> &[Value] {
        &self.data
    }
}
```

One thing to note here is we're returning a reference to a **slice** not a Vec, to indicate that we're not transferring ownership and also to leave us open in future for modification (anything that is coerced to a slice can be returned/stored in the struct).

The first major task we want is parsing. We have a slice of bytes (u8's) of arbitrary length and we need to parse this using the IDX protocol. Assuming we're giving data in the correct format (and in this case for our toy library we don't care about error handling and should just crash if something is wrong), then we can just use the **From\<&[u8]>** trait:

```rust
impl From<&[u8]> for FileContents {
    fn from(bytes: &[u8]) -> Self {
        // implementation goes here
    }
}
```

As for the implementation, we need to do the following sequence of actions:
1. Parse the first **4** bytes as the magic number.
2. Get the data type and number of dimensions from the magic number.
3. For each dimension, read the next 4 bytes from the slice, giving us the size in that dimension.
4. Read the rest of the data, parsing into the correct data type.

We'll look at these one at a time.

For step #1, we have the MagicNumber type, so we'll delegate our actual parsing to that type, but first we need to take the first 4 bytes from the slice. We can do this in Rust with a method on slices called **split_at** which takes an index and returns all elements **below** that index as one slice, and the rest as a second slice.

Once we've split the slice, we need to turn that first (now 4 byte long) slice into an array, specifically **[u8; 4]**. This is because we've implemented From<[u8; 4]> for MagicNumber so we need to convert before we can turn it into a MagicNumber. We can do this in Rust with the try_into implementation (which we just unwrap).

Finally we can use the From implementation to turn it into a MagicNumber instance, which will fail if the slice isn't right. Overall the code to cover step 1 is:

```rust
let (magic_number, bytes) = bytes.split_at(4);
let magic_number: [u8; 4] = magic_number.try_into().unwrap();
let magic_number: MagicNumber = magic_number.into();
```

For step #2, we implemented From\<MagicNumber> for (DataType, u8), as in we already implemented a way to decompose the struct into its parts, so we can just do:

```rust
let (data_type, num_dimensions) = magic_number.into();
```

Step #3 is to read the sizes of the dimensions. We can do this as a fold actually, we take the iterator that runs from 0 to the number of dimensions and call fold on it, with a preallocated Vec. In the callback function we take the sizes Vec and the remaining bytes slice. This part looks as follows:

```rust
let (sizes, bytes) = (0..num_dimensions).fold(
    (Vec::with_capacity(num_dimensions as usize), bytes),
    |(mut sizes, bytes), _| {
        // update sizes and bytes and return here
    }
};
```

To take off the first 4 bytes, we use the split_at function again, assert that it's the correct length, then parse it as a **u32** before pushing it to the sizes vector.

The structure from what we already know then is:

```rust
let (size, bytes) = bytes.split_at(4);
assert!(size.len() == 4);
???
sizes.push(size);
(sizes, bytes)
```

In the ??? line, we just need to know how to convert the size (which is a byte slice) into a u32. Rust helpfully provides an associated method for this called **from_be_bytes** which takes a [u8; 4] that it assumes is in **big endian** ordering and reinterprets it as a u32. This function only runs on a fixed 4 element array, and we have a slice so we need to use the try_into implementation thusly:

```rust
let size = u32::from_be_bytes(size.try_into().unwrap());
```

Now we have a vector of sizes for the dimensions, which is half the battle!. We just need to read the data in now.

Firstly we need to know how many values we're reading. We know that the data is basically an N-dimensional array and each instance of a given dimension is the same length, which means that the total number of elements in the vector/matrix/etc. is the product of all the sizes.

We can do this in Rust easily with the Iterator API:

```rust
let number_values: u32 = sizes.iter().cloned().product();
```

We can then get the number of bytes per value (which tells us how we need to split the slice), from the DataType:

```rust
let bytes_per_value: u8 = data_type.number_of_bytes();
```

For parsing the data, we are essentially following the same pattern as we did for parsing the dimension sizes, that is:

1. Fold from 0 to number_values
2. For each, we split the byte slice so that we break off bytes_per_value bytes
3. Assert that the split off byte slice is the correct length
4. Parse the bytes as the appropriate DataType, parse using the From<(&[u8], DataType)> implementation of Value
5. Push this into the fold state which is returned

In its entirety this looks as follows:

```rust
let (data, _) = (0..number_values).fold(
    (Vec::with_capacity(number_values as usize), bytes),
    |(mut data, bytes), _| {
        let (value, bytes) = bytes.split_at(bytes_per_value as usize);
        assert!(value.len() == bytes_per_value as usize);
        let value: Value = (value, data_type.clone()).into();
        data.push(value);
        (data, bytes)
    },
);
```

That's it, we can now construct FileContents from the sizes and data we parsed!

There is however another thing we will need to be able to do, we need to be able to index into this data. Not strictly necessary because we can access the flat data slice, but it will let us instead of writing in calling code:

```rust
file_contents.data()[(image_index * width * height) + (y * width) + x]
```

we want to simply be able to write:

```rust
file_contents[(image_index, y, x)]
```

In Rust this operation, as with all other operations, is implemented via a trait. In this case, it's the **Index** trait which takes a generic parameter indicating the type of key, and requires two items:

1. The type of output
2. An index function that takes a key of the key type, and returns a **reference** to the output type.

We can only index correctly into a 3 dimensional data, since we're giving a key with 3 parts, so we need to assert the sizes vector is 3.

We then assert that each individual index is less than the corresponding dimension's size.

If the provided key passes these assertions then the index into the flat data can be calculated as:

$$ index(idx1, idx2, idx3) = (idx1 \times size(2) \times size(1)) + (idx2 \times size(1)) + idx3 $$

And finally we can index into the data array to get a reference to the value at that point.

In its entirety it looks like follows:

```rust
impl Index<(u32, u32, u32)> for FileContents {
    type Output = Value;

    fn index(&self, idx: (u32, u32, u32)) -> &Self::Output {
        assert_eq!(self.sizes.len(), 3);
        let (idx1, idx2, idx3) = idx;
        assert!(self.sizes[0] > idx1);
        assert!(self.sizes[1] > idx2);
        assert!(self.sizes[2] > idx3);
        let idx = (idx1 * self.sizes[2] * self.sizes[1]) + (idx2 * self.sizes[1]) + idx3;
        &self.data[idx as usize]
    }
}
```

---
# Converting FileContents to input array #

Now we've been able to load the IDX files and have FileContents structures for all 4 of them. 2 of them will be 3-dimensional (the images) and 2 of them will be 1-dimensional (the labels).

We need to convert these into ndarray::Array types for use with our network, and this first function will be focussed on converting the **image** data to an Array of f64 elements.

Firstly we can assign meaning to the dimensions in the FileContents structure. The dimensions for the images are:

- **Dimension 0** => the image axis, allows selecting the image we're referring to
- **Dimension 1** => the y coordinate, allows selecting a row in the image
- **Dimension 2** => the x coordinate, allows selecting an individual element inside a row

So in code we do:

```rust
let number_images = contents.sizes()[0] as usize; let height = contents.sizes()[1] as usize; let width = contents.sizes()[2] as usize;
```

Next we need to take the data and transform them into a Vec\<f64> so that we can construct an array from them. We can do this by getting an iterator to the data, then mapping the iterator using a function, then collecting the results into a Vec.

For the mapping function, it will take a Value (which we know will be Value::U8), extracts the u8 from inside the enum, and then converts that from a 0-255 integer into a 0.0-1.0 float by dividing by 255.0. The whole procedure looks as follows:

```rust
let data = contents .data() .into_iter() .map(|value| { let value: u8 = value.into(); value as f64 / 255.0 }) .collect();
```

Now we have the Vec, we can create an Array from it with the standard from_shape_vec function we've been using. The number of rows is the number of images, and the number of columns will be the image width multiplied by the image height:

```rust
Array::from_shape_vec((number_images, width * height), data).unwrap()
```

---
# Converting FileContents to target array #

Parsing the 1-dimensional FileContents that contain the labels is both simpler but also more complex. Simpler because there's only one dimension to worry about, but more complex because the labels are given as integers in the range 0-9, however for a classification network to do a good job, we want to **one-hot encode** the labels.

What this means is that we need to take the single value, and create 10 columns for it. In those columns, they'll all be 0, except for the one column at the correct index.

Below we can show this mapping:

- 0 => [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
- 1 => [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
- 2 => [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
- 3 => [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
- 4 => [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
- 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
- 6 => [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
- 7 => [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
- 8 => [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
- 9 => [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]

What we want is a mapping where each element of the file contents maps to **10** elements in the output vector. We can do this by using the flat_map function instead of a regular map, and returning an iterator that produces 10 elements for each input element.

In order to create an iterator we can use the **chain** function to chain together 10 iterators. Each of the iterators will produce only a single value which will be either a 0 or a 1 indicating whether the input value is the associated value.

Then we collect into an array again, this time with 10 columns.

The whole thing looks like follows:

```rust
let number_images = contents.sizes()[0] as usize; let data = contents .data() .into_iter() .flat_map(|value| { let value: u8 = value.into(); let value_0 = std::iter::once((value == 0) as u8 as f64); let value_1 = std::iter::once((value == 1) as u8 as f64); let value_2 = std::iter::once((value == 2) as u8 as f64); let value_3 = std::iter::once((value == 3) as u8 as f64); let value_4 = std::iter::once((value == 4) as u8 as f64); let value_5 = std::iter::once((value == 5) as u8 as f64); let value_6 = std::iter::once((value == 6) as u8 as f64); let value_7 = std::iter::once((value == 7) as u8 as f64); let value_8 = std::iter::once((value == 8) as u8 as f64); let value_9 = std::iter::once((value == 9) as u8 as f64); value_0 .chain(value_1) .chain(value_2) .chain(value_3) .chain(value_4) .chain(value_5) .chain(value_6) .chain(value_7) .chain(value_8) .chain(value_9) }) .collect(); Array::from_shape_vec((number_images, 10), data).unwrap()
```

---
# Building the network #

Now we have the data prepared and can build and train the network!

Since we've seen several networks so far, it should be fairly obvious if I just paste the whole setup code:

```rust
// hyperparameters 
const LEARNING_RATE: f64 = 0.001; const EPOCHS: u32 = 100000; const EVAL_EVERY: u32 = 2; const BATCH_SIZE: usize = 256; const SEED: u64 = 42; 
// Prepare Network 
let mut network = Network::new( vec![ Layer::new_with_seed(65, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED), Layer::new_with_seed(10, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED), ], Loss::new(SoftmaxCrossEntropy::new_boxed()), ); 
let mut optimiser = SGD::new(LearningRateFixed::new(LEARNING_RATE));
```

We're using a fixed learning rate with standard SGD optimiser. We use SoftmaxCrossEntropy loss because this is a classification problem.

This network uses two layers only, one with 65 neurons and an output layer with 10.

---
# Calculating accuracy #

In order to determine the accuracy of the network, we need to look at each of the rows of the predictions, alongside the targets for those predictions that we expect, we can use a zip for this. Then we need to filter the zipped iterator of tuples to only include elements where the prediction matches the target. We can then use the **count** function on the iterator to count the correct predictions:

```rust
let correct_count = predictions .rows() .into_iter() .zip(targets.rows().into_iter()) .filter(|(prediction, target)| { 
// perform check here 
}) .count();
```

We can't however just do (prediction == target) here, because the targets are guaranteed to be 0 and 1 values (and only have a single 1), but the predictions are more fuzzy than that.

From the predictions, what we actually want to do is take the **highest** element as the guess. So to get the predicted number, we need to go over the elements and indices of the prediction row and track the index of the maximum element, which is done as so:

```rust
let mut predicted_max = f64::MIN;
let mut predicted_number = 0;
prediction.iter().enumerate().for_each(|(idx, value)| {
    if *value > predicted_max {
        predicted_max = *value;
        predicted_number = idx;
    }
});
```

Finding the target number is easier, we're just looking for the index that corresponds to the 1. This can be done with the following code:

```rust
let target_number = target
    .iter()
    .enumerate()
    .filter(|(_, value)| **value == 1.0)
    .map(|(idx, _)| idx)
    .next()
    .unwrap();
```

And finally we can check the two for equality.

The accuracy then is calculated using the standard percentage calculation of (correct/total) * 100.

---
# Results #

After coding all of this up and running it, we can see that it performs incredibly well on the training data, and not bad on the testing data:

```
Accuracy (training): 99.61833333333333%
Accuracy (testing): 97.59%
```

This is using only a simple feed forward neural network so once we learn about convolutional networks we should be able to achieve even greater accuracy!