The second part of using textures in Vulkan will be creating the texture sampler and binding descriptor for the shader to be able to use the image.

---
# Renaming create_texture

After working through the [[17. Textures Part 1|previous section]] we have ended up with a function called **create_texture** but what it actually does is only create the texture **image** so we'll go ahead and rename the function.

```rust
fn create_texture_image(
    instance: &Instance,
    device: &Device,
    physical_device: PhysicalDevice,
    transfer_queue: Queue,
    transfer_command_pool: CommandPool,
    path: impl AsRef<Path> + Clone + Display,
) -> Result<Texture>
```

Then we'll go ahead and create another function called **create_texture** which will simply call this one for now (will do additional work later).

```rust
fn create_texture(
    instance: &Instance,
    device: &Device,
    physical_device: PhysicalDevice,
    transfer_queue: Queue,
    transfer_command_pool: CommandPool,
    path: impl AsRef<Path> + Clone + Display,
) -> Result<Texture> {
	create_texture_image(
	    instance,
	    device,
	    physical_device,
	    transfer_queue,
	    transfer_command_pool,
	    path,
	)
	.with_context(|| format!("Attempting to create a texture image at path {path}"))?;
}
```

---
# Creating Image View

We have the texture image now, but need an image **view** for that (Vulkan works with views of images and buffers!).

We'll go ahead and change the content so that we store the image in a variable and then create a view from it.

```rust
let texture_image = create_texture_image(
	instance,
	device,
	physical_device,
	transfer_queue,
	transfer_command_pool,
	path.clone(),
)
.with_context(|| format!("Attempting to create a texture image at path {path}"))?;

let texture_image_view = create_image_view(
	texture_image.image,
	Format::R8G8B8A8_UNORM,
	ImageAspectFlags::COLOR,
	device,
)
.with_context(|| {
	format!("Attempting to create an image view for texture image loaded from path {path}")
})?;

Ok(texture_image)
```

---
# Adding Image View To Texture

Next we get to add the image view we just created to the Texture struct. We need this for cleanup so the cleanup method can destroy it.

Let's first rename the existing Texture struct to TextureImage.

```rust
struct TextureImage {
    image: Image,
    image_memory: DeviceMemory,
    _width: u32,
    _height: u32,
}
```

And make a new Texture struct which will hold the image and for now an image view.

```rust
struct Texture {
    image: TextureImage,
    view: ImageView
}
```

We'll have the **create_texture_image** function return TextureImage now instead of Texture, and have **create_texture** return the actual Texture.

```rust
Ok(Texture {
	image: texture_image,
	view: texture_image_view
})
```

Finally change the cleanup function due to the structural change.

```rust
for texture in textures {
	device.destroy_image_view(texture.view, None);
	device.destroy_image(texture.image.image, None);
	device.free_memory(texture.image.image_memory, None);
}
```

---
# Creating The Sampler

In order to make the texture available to the shader, we need to create a sampler with the Device.

Let's make a function that will create a sampler for us!

```rust
fn create_texture_sampler(device: &Device) -> Result<Sampler> {
    unsafe {
        device
            .create_sampler(
                &SamplerCreateInfo::builder()
                    .mag_filter(Filter::LINEAR)
                    .min_filter(Filter::LINEAR)
                    .address_mode_u(SamplerAddressMode::REPEAT)
                    .address_mode_v(SamplerAddressMode::REPEAT)
                    .address_mode_w(SamplerAddressMode::REPEAT)
                    .border_color(BorderColor::INT_OPAQUE_BLACK)
                    .unnormalized_coordinates(false)
                    .mipmap_mode(SamplerMipmapMode::LINEAR)
                    .mip_lod_bias(0.0)
                    .min_lod(0.0)
                    .max_lod(0.0)
                    .anisotropy_enable(true)
                    .max_anisotropy(16.0),
                None,
            )
            .context("creating a texture sampler.")
    }
}
```

There's not that much going on here - we have the usual unsafe block to call the create_sampler method on the Device. We have the usual None for allocation callbacks since we're not using custom allocation. We also have a context on the Result.

It will be useful however to go through the various settings here on the sampler.

## mag_filter

This setting is used to tell the sampler how to handle the case where a single texel takes up many pixels on the screen. The filtering to use when it's larger on the screen.

In our case we use linear filtering.

This can be thought of I think as how we take the edge off that pixelisation effect we get when texels are too large on the screen.

As a summary:

- **Linear** => This will blend between texels to create a smooth look
- **Nearest** => This will choose the closest texel and create a pixelated look

## min_filter

This is the opposite of the mag_filter and tells the sampler what filtering to apply when a given texel takes up less than a pixel on screen.

Again we use linear here.

Minification can use mipmapping based on distance from the texture, but we don't use that here - we're only using a single mip.

## address_mode_{u,v,w}

These tell the sampler how to handle the addressing when we sample out of bounds of the texture. Here we use repeat so that the texture will just tile/repeat in the three axes.

The possible values we might use here are:

- **REPEAT** => This causes the texture to tile/repeat in the axis
- **MIRRORED_REPEAT** => This causes the texture to tile/repeat in a mirrored fashion in the axis
- **CLAMP_TO_EDGE** => This causes the edge pixel to be used if outside of the bounds
- **MIRRORED_CLAMP_TO_EDGE** => This is like clamp to edge but mirrored
- **CLAMP_TO_BORDER** => This uses whatever the border color is set as

## border_color

Defines the color for when CLAMP_TO_BORDER is used!

Here we used solid black as the color.

## unnormalized_coordinates

Defines whether unnormalized coordinates are used or not. If it's false then that means our coordinates are going to be **normalized**

## mipmap_mode

Specifies how the blending works between two mip levels.

Linear means a linear interpolation between the mip levels as you approach or move away.

Again we can use either **NEAREST** or **LINEAR** to pick either the nearest mip level or blend between them.

Blending will be more expensive, but if you use nearest you'll see sudden swap between mip levels. Might not be an issue if there's enough mip levels though!

## mip_lod_bias

This allows for specifying an offset when calculating which mip level to use.

## min_lod/max_lod

Specifies the minimum and maximum lod levels.

We're not using them here though so they're 0.

## anisotropy_enable

Defines whether anisotropic filtering is enabled.

This will affect blending between texels when viewing from oblique angles, for example closer to parallel with the floor.

## max_anisotropy

This is the maximum number of samples taken around a texel when using anisotropic filtering.

---
# Creating/Destroying Sampler

Let's go ahead and call the function to create the sampler, and then also add it to cleanup.

First creation, add the following into main:

```rust
// create a sampler
let sampler = create_texture_sampler(&logical_device).context("Creating a texture sampler to use")?;
```

Next we can add it to the cleanup method and pass it in.

```rust
device.destroy_sampler(sampler, None);
```

---
# Supporting Anisotropic Filtering

Running the code so far will cause a validation error because we haven't specified when creating the device that anisotropic filtering is enabled.

We can go back to where we are creating the logical device and ensure that this is enabled with the following line added to the **CreateDeviceInfoBuilder**.

```rust
.enabled_features(&PhysicalDeviceFeatures::builder().sampler_anisotropy(true)),
```

However we need to make sure that the actual physical device we picked supports this feature at the hardware level.

We can do this by going back to the **get_physical_device** method that picks it for us and adding a check for the sampler anisotropy feature.

We'll add this just inside the valid surface info check.

```rust
if surface_info.is_valid() {
	let device_features =
		unsafe { instance.get_physical_device_features(physical_device) };
		
	if device_features.sampler_anisotropy != 0 {
		return Ok((physical_device, queue_family_indices, surface_info));
	}
}
```

Note that these device features are 32 bit boolean values. That is, they can only ever be 0 or 1, however aren't **bool** type but are **u32** type. In Rust this means we need to use a comparison operator (!= 0) as implicit conversions are disallowed.

---
# Allocating Sampler Descriptors

We have a descriptor pool currently allocating uniform buffer descriptors, but we will want to also allocate space for texture samplers as well.

For this learning process, we can go ahead and assume that each object only has a single texture.

We can re-use the same descriptor pool, but we'll make different descriptor sets for the textures we have so we can mix and match any texture with any uniform buffer.

The create_descriptor_pool then looks as follows after changing.

```rust
fn create_descriptor_pool(
    device: &Device,
    uniform_count: u32,
    texture_count: u32,
) -> Result<DescriptorPool> {
    unsafe {
        device
            .create_descriptor_pool(
                &DescriptorPoolCreateInfo::builder()
                    .max_sets(uniform_count + texture_count)
                    .pool_sizes(&[
                        *DescriptorPoolSize::builder()
                            .ty(DescriptorType::UNIFORM_BUFFER)
                            .descriptor_count(uniform_count),
                        *DescriptorPoolSize::builder()
                            .ty(DescriptorType::COMBINED_IMAGE_SAMPLER)
                            .descriptor_count(texture_count),
                    ]),
                None,
            )
            .context("Failed to create a descriptor pool.")
    }
}
```

Then we need to create a descriptor set layout defining the descriptors in that set. Let's create a new method called **create_texture_descriptor_set_layout** which will be pretty similar to the existing create_descriptor_set_layout function (probably rename that to create_uniform_buffer_descriptor_set_layout maybe).

```rust
fn create_descriptor_set_layout(device: &Device) -> Result<DescriptorSetLayout> {
    unsafe {
        device
            .create_descriptor_set_layout(
                &DescriptorSetLayoutCreateInfo::builder().bindings(&[
                    *DescriptorSetLayoutBinding::builder()
                        .descriptor_type(DescriptorType::UNIFORM_BUFFER)
                        .descriptor_count(1)
                        .stage_flags(ShaderStageFlags::VERTEX),
                    *DescriptorSetLayoutBinding::builder()
                        .descriptor_type(DescriptorType::COMBINED_IMAGE_SAMPLER)
                        .descriptor_count(1)
                        .stage_flags(ShaderStageFlags::FRAGMENT)
                        .binding(1),
                ]),
                None,
            )
            .context("Failed to create a descriptor set layout.")
    }
}
```

Now we can allocate a number of texture descriptor sets - one per texture.

```rust
let texture_descriptor_set_layout = create_texture_descriptor_set_layout(&logical_device)?;

let texture_descriptor_sets = allocate_descriptor_sets(
	&logical_device,
	descriptor_pool,
	texture_descriptor_set_layout,
	textures.len(),
)?;
```

Finally we need to pass these in along with the textures when we update the descriptor sets so we can associate each of the texture descriptor sets with a texture/image view. Similar to the uniform buffer descriptors associating with a uniform buffer.

```rust
let texture_infos = textures
	.iter()
	.map(|texture| {
		[DescriptorImageInfo::builder()
			.sampler(sampler)
			.image_view(texture.view)
			.image_layout(ImageLayout::SHADER_READ_ONLY_OPTIMAL)
			.build()]
	})
	.collect::<Vec<_>>();
```

And then extend the list of writes that we have to the descriptor sets.

```rust
writes.extend(
	texture_infos
		.iter()
		.zip(texture_sets)
		.map(|(texture_info, set)| {
			WriteDescriptorSet::builder()
				.dst_set(*set)
				.dst_binding(1)
				.descriptor_type(DescriptorType::COMBINED_IMAGE_SAMPLER)
				.image_info(texture_info)
				.build()
		}),
);
```

---
# Adding Texture To Mesh

Next we can go ahead and allow the Mesh to have a texture specified.

First we need to go ahead and add the texture reference into the mesh. We can just go ahead and add the index of the texture in the array to use.

```rust
struct Mesh {
    vertex_buffer: Buffer,
    vertex_buffer_memory: DeviceMemory,
    index_buffer: Buffer,
    index_buffer_memory: DeviceMemory,
    index_count: usize,
    push_model: PushModel,
    texture_index: usize
}
```

Then we go and change the create_mesh function so that we can specify this for creation and call it. Don't need to show this, it's fairly self explanatory.

---
# Binding Texture Descriptor Set For Call

When doing the draw call for the Mesh, let's throw the texture set in there!

Add these parameter to the record_command_buffers function.

```rust
texture_descriptor_sets: &[DescriptorSet],
```

Now binding the descriptor sets changes to this.

```rust
device.cmd_bind_descriptor_sets(
	command_buffer,
	PipelineBindPoint::GRAPHICS,
	pipeline_layout,
	0,
	&[descriptor_set, texture_descriptor_sets[mesh.texture_index]],
	&[],
);
```

Needs routing through from main appropriately though!

---
# Updating Pipeline Layout

If trying to run this, there's an error saying we're binding too many sets for the draw call. So need to change the pipeline layout to specify we have a second one.

```rust
let pipeline_layout = create_pipeline_layout(
	&logical_device,
	&[
		uniform_buffer_descriptor_set_layout,
		texture_descriptor_set_layout,
	],
)?;
```

---
# Adding Texture Coords To Vertex

Currently we have position and color attributes on each vertex, but when using textures will also need to store the texture coordinates (uv).

Adding the coordinates to the Vertex struct.

```rust
#[repr(C)]
struct Vertex {
    position: [f32; 3], // offset 0
    color: [f32; 3],    // offset 12
    tex: [f32; 2], // offset 24
}
```

When defining the vertices for the mesh, we have something like this.

```rust
[
	Vertex {
		position: [-0.2, -0.2, -1.2],
		color: [0.0, 1.0, 0.0],
		tex: [1.0, 1.0]
	},
	Vertex {
		position: [0.2, 0.2, -1.2],
		color: [0.0, 1.0, 0.0],
		tex: [1.0, 0.0]
	},
	Vertex {
		position: [-0.2, 0.2, -1.2],
		color: [0.0, 1.0, 0.0],
		tex: [0.0, 0.0]
	},
	Vertex {
		position: [0.2, -0.2, -1.2],
		color: [0.0, 1.0, 0.0],
		tex: [0.0, 1.0]
	},
]
```

Finally need to add this attribute to the vertex attributes defined in **create_graphics_pipeline**.

```rust
let attribute_descriptions = [
*VertexInputAttributeDescription::builder().format(Format::R32G32B32_SFLOAT),
	*VertexInputAttributeDescription::builder()
		.location(1)
		.format(Format::R32G32B32_SFLOAT)
		.offset(12),
	*VertexInputAttributeDescription::builder()
		.location(2)
		.format(Format::R32G32B32_SFLOAT)
		.offset(24),
];
```

---
# Modifying Shaders

All the code *should* be done we need to go ahead and modify the shaders now so that they sample from the sampler and use the UV coords specified.

First the fragment shader, adding the tex coordinate as an input and adding an output for the fragment shader to get the interpolated UV coordinate.

```rust
layout(location = 2) in vec2 tex;
layout(location = 1) out vec2 fragTex;
```

And in the main function of the shader, tell it to write the input to output to be passed along.

```rust
fragTex = tex;
```

Next in the fragment shader add an input for the tex coordinate as well.

```rust
layout(location = 1) in vec2 fragTex;
```

How to use the UV to sample from the texture?

We need to go and add a texture sampler to the fragment shader.

```rust
layout(set = 1, location = 0) uniform sampler2D textureSampler;
```

Finally in the main function where it's calculating the color - we'll need to blend the texture color with the color we want (add color tint).

```rust
outColour = texture(textureSampler, fragTex) * vec4(fragColour, 1.0);
```

---
# Oh No! - An Issue With Naga

After implementing all of the above with combined texture descriptors. It turns out that the Rust library we're using to perform the GLSL to SPIR-V compilation is actually not supporting combined image samplers.

Therefore need to split out the sampler from the texture from the point of view of the bindings/descriptors.

In the fragment shader we need to first replace the uniform sampler with **two** uniforms for the texture and the sampler independently.

```rust
layout(set = 1, binding = 0) uniform texture2D texture;
layout(set = 1, binding = 1) uniform sampler textureSampler;
```

When we want to sample the texture we can then go ahead and make the sampler2D at that point to pass to the texture function.

```rust
outColour = texture(sampler2D(texture, textureSampler), fragTex) * vec4(fragColour, 1.0);
```

Running this now will give an error from Vulkan about our descriptor layout/descriptor sets not being valid so we need to go ahead and correct those to account for the new bindings.

```rust
fn create_texture_descriptor_set_layout(device: &Device) -> Result<DescriptorSetLayout> {
    unsafe {
        device
            .create_descriptor_set_layout(
                &DescriptorSetLayoutCreateInfo::builder().bindings(&[
                    *DescriptorSetLayoutBinding::builder()
                        .descriptor_type(DescriptorType::SAMPLED_IMAGE)
                        .descriptor_count(1)
                        .stage_flags(ShaderStageFlags::FRAGMENT)
                        .binding(0),
                    *DescriptorSetLayoutBinding::builder()
                        .descriptor_type(DescriptorType::SAMPLER)
                        .descriptor_count(1)
                        .stage_flags(ShaderStageFlags::FRAGMENT)
                        .binding(1),
                ]),
                None,
            )
            .context("Failed to create a descriptor set layout.")
    }
}
```

Note that to match the shader, we go ahead and set:
- Binding **0** => The image view we're sampling (texture).
- Binding **1** => The sampler itself that we'll be using.

Now that the layout has been changed, we need to actually change the **update_descriptor_sets** function to write correctly to those bindings.

We can just change the appropriate section of code to the following.

```rust
writes.extend(
	texture_infos
		.iter()
		.zip(texture_sets)
		.flat_map(|(texture_info, set)| {
			[
				WriteDescriptorSet::builder()
					.dst_set(*set)
					.dst_binding(0)
					.descriptor_type(DescriptorType::SAMPLED_IMAGE)
					.image_info(texture_info)
					.build(),
				WriteDescriptorSet::builder()
					.dst_set(*set)
					.dst_binding(1)
					.descriptor_type(DescriptorType::SAMPLER)
					.image_info(texture_info)
					.build(),
			]
			.into_iter()
		}),
);
```

Note that both the image view and the sampler come from the image info provided.

---
# Testing

We can run the thing now and see that the rotating quads are now textured.

![[texture-tinted.gif]]

We can also go ahead and modify the shader to remove the tint and see the colours of the texture as they are. Note that this texture we picked isn't square so is being squashed to fit on the quad.

![[texture-untinted.gif]]