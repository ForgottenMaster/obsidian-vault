Currently our vertex buffer is residing on the GPU but in a section of memory that is visible to the CPU. This incurs an overhead on the GPU so that it's not able to process that memory as fast as it could if it knew the CPU could not see it.

---
# Memory Types #

In Vulkan there are various pools of memory with different properties that we can use to allocate buffer memory. This diagram shows a relationship between the various areas that memory and caching can reside

![[Memory Types.jpeg]]

The fastest area for memory, as mentioned already, is the area labelled "device local". The GPU can run this memory as fast as it can because it knows it's in sole charge of the data and the CPU cannot see, nor modify it without going through the GPU first.

But how can we get data into that memory if we can't see it? This is where **staging buffers** come in.

---
# Staging Buffers #

A staging buffer is a (potentially) temporary buffer that *is* host visible, that we can write our data into from the application side. From there, we tell Vulkan to transfer our data from that buffer, into a buffer backed by device local memory where it'll be used longer term.

The sequence of operations therefore to get data onto device local memory is as follows:

1. Allocate a buffer of the appropriate size, with the usage type of **TRANSFER_SRC** set.
2. Allocate memory of the correct size from a supported type, with the properties **HOST_VISIBLE** and **HOST_COHERENT** set.
3. Bind the memory allocated in step 2, to the buffer allocated in step 1.
4. Map, write the data, and unmap the staging buffer to get the data into this memory from the application.
5. Allocate a buffer of the appropriate size, with the usage type of **TRANSFER_DST** set.
6. Allocate memory of the correct size from a supported type, with the property **DEVICE_LOCAL** set.
7. Bind the memory allocated in step 6, to the buffer allocated in step 5.
8. Allocate a command buffer.
9. Record a copy operation into the command buffer.
10. Submit the command buffer to a queue capable of transfer operations.
11. (Optional) Free the command buffer.
12. (Optional) Free the staging buffer memory. 
13. (Optional) Free the staging buffer.

Note that steps 11, 12, and 13 are marked as optional because they may be reused through the lifetime of an application. They'll need to be freed/destroyed by the application teardown stage though.

We'll then go ahead and implement this sequence of steps, assuming that the staging buffer and copy command buffer are only good for one staging operation, meaning we'll clean them up after the copy is finished.

---
# Function to Make a Buffer #

The first step will be to take our code that we had in our **create_vertex_buffer** function and make it generic, so that we can make any kind of buffer with it.

This is mostly a simple exercise, the signature will look as follows

```rust
fn create_buffer(
    instance: &Instance,
    device: &Device,
    physical_device: PhysicalDevice,
    usage: BufferUsageFlags,
    memory_property_flags: MemoryPropertyFlags,
    size: DeviceSize,
) -> Result<(Buffer, DeviceMemory)>
```

We pass into the generic function the information it needs
- **usage**: This is the buffer usage, such as VERTEX_BUFFER, TRANSFER_SRC, etc.
- **memory_property_flags**: Indicates the type of memory we want to allocate from, for example DEVICE_LOCAL, HOST_VISIBLE, etc.
- **size**: The size (in bytes) of the buffer to allocate.

There isn't much to write home about on this function - it's the same as we had for our vertex buffer, but with some hard-coded things replaced with parameters. As such, I'll just list the entire code

```rust
unsafe {
    // create a buffer handle of the right size and type.
    let buffer = device.create_buffer(
        &BufferCreateInfo::builder()
            .size(size)
            .usage(usage)
            .sharing_mode(SharingMode::EXCLUSIVE),
        None,
    )?;

    // get buffer memory requirements plus the memory properties of our physical device.
    let memory_requirements = device.get_buffer_memory_requirements(buffer);
    let memory_properties = instance.get_physical_device_memory_properties(physical_device);

    // find a valid memory type index to use.
    let memory_type_index = find_valid_memory_type_index(
        memory_properties,
        memory_requirements,
        memory_property_flags,
    )
    .ok_or_else(|| anyhow!("Failed to get a valid memory type for buffer."))?;

    // allocate memory.
    let buffer_memory = device
        .allocate_memory(
            &MemoryAllocateInfo::builder()
                .allocation_size(memory_requirements.size)
                .memory_type_index(memory_type_index as u32),
            None,
        )
        .context("Failed to allocate buffer memory.")?;

    // bind buffer memory.
    device
        .bind_buffer_memory(buffer, buffer_memory, 0)
        .context("Failed to bind buffer memory to the buffer.")?;

    // return.
    Ok::<_, Error>((buffer, buffer_memory))
}
.context("Error when trying to create a buffer of some type.")
```

---
# Function to Make a Staged Buffer #

Next, we'll make a reusable function that will be able to take a list of some data type T as a slice, and will push that data to a buffer residing on device local memory via a staging buffer.

We'll essentially be implementing steps 1-13 above in code.

Firstly let's take a look at the signature of this function

```rust
fn create_staged_buffer<T>(
    instance: &Instance,
    device: &Device,
    physical_device: PhysicalDevice,
    elements: &[T],
    usage: BufferUsageFlags,
    transfer_command_pool: CommandPool,
    transfer_queue: Queue,
) -> Result<(Buffer, DeviceMemory)>
```

Notably this function is generic over any type T, and takes a slice of T's to push to the GPU. Additionally the caller must provide a command pool created on a queue family that supports transfer operations, and a specific queue to push the copy command buffer to.

The first thing required is to determine how many bytes we need to allocate. This is quite simple as we can get the size of a T from std::mem::size_of, and we know how many T's there are. We must, however also convert this to a DeviceSize, which is a type alias for u64, because size_of returns a usize.

```rust
let size = (mem::size_of::<T>() * elements.len()) as DeviceSize;
```

Next we need to create 2 buffers, and backing memory storage. One will be for the staging buffer and will be dropped after copying the data, which will require a usage flag of **TRANSFER_SRC**, and will require a memory type that is **HOST_VISIBLE** and **HOST_COHERENT**.

The device local buffer we will be returning needs to have **TRANSFER_DST**, and the memory type needs to be **DEVICE_LOCAL**. Written as code this would be

```rust
let (staging_buffer, staging_buffer_memory) = create_buffer(
    instance,
    device,
    physical_device,
    usage | BufferUsageFlags::TRANSFER_SRC,
    MemoryPropertyFlags::HOST_VISIBLE | MemoryPropertyFlags::HOST_COHERENT,
    size,
)
.context("Failed to create staging buffer.")?;

let (gpu_buffer, gpu_buffer_memory) = create_buffer(
    instance,
    device,
    physical_device,
    usage | BufferUsageFlags::TRANSFER_DST,
    MemoryPropertyFlags::DEVICE_LOCAL,
    size,
)
.context("Failed to create GPU buffer.")?;
```

We then go ahead and copy the data from our slice of T's to the allocated staging buffer memory using the method that we previously used for our vertex buffer (map, write, unmap)

```rust
let write_ptr = device
    .map_memory(staging_buffer_memory, 0, size, MemoryMapFlags::empty())
    .context("Failed to map the staging buffer memory.")? as *mut T;
ptr::copy_nonoverlapping(elements.as_ptr(), write_ptr, elements.len());
device.unmap_memory(staging_buffer_memory);
```

Our data is now residing in our staging buffer so we can tell Vulkan to transfer it to our device local buffer. To do this we first need to allocate a command buffer to record the copy operation into. This needs to be a primary command buffer because we will be submitting it directly to the queue.

```rust
let command_buffer = device
    .allocate_command_buffers(
        &CommandBufferAllocateInfo::builder()
            .command_pool(transfer_command_pool)
            .level(CommandBufferLevel::PRIMARY)
            .command_buffer_count(1),
    )
    .context("Failed to allocate a staging transfer command buffer.")?[0];
```

Before we can record any commands to the buffer, we need to tell Vulkan we are starting to record to that particular buffer. In this case we will use the **ONE_TIME_SUBMIT** flag since we will be returning the command buffer to the pool once the copy operation is over.

```rust
device
    .begin_command_buffer(
        command_buffer,
        &CommandBufferBeginInfo::builder().flags(CommandBufferUsageFlags::ONE_TIME_SUBMIT),
    )
    .context("Failed to begin recording the command buffer.")?;
```

Now that we've told Vulkan we're recording our commands into that buffer, we can go ahead and issue our command. In this case we have only a single command to do a buffer copy operation.

Vulkan expects the source buffer, and the destination buffer, but also we can specify offsets in each to copy with. In our case though, we copy from the beginning of the source buffer (our staging buffer), to the beginning of the destination buffer (our device local buffer).

```rust
device.cmd_copy_buffer(
    command_buffer,
    staging_buffer,
    gpu_buffer,
    &[*BufferCopy::builder().size(size)],
);
```

With that, we can tell Vulkan we're done recording commands into the command buffer

```rust
device
    .end_command_buffer(command_buffer)
    .context("Failed to end recording the command buffer.")?;
```

We will now submit the buffer to the transfer queue, much like we submitted our drawing buffer to the graphics queue for rendering. We could provide a fence or semaphore to this operation if needed, but in our case we will push the buffer to the queue, and then *wait* until the queue is finished processing.

Blocking the CPU like this isn't necessarily a good idea, but it makes the code simpler in this case!

```rust
// submit the copy operation to the transfer queue.
let command_buffers = [command_buffer];
let submit_infos = [*SubmitInfo::builder().command_buffers(&command_buffers)];
device
    .queue_submit(transfer_queue, &submit_infos, Fence::null())
    .context("Failed to submit the command buffer to the queue.")?;

// block the thread until the copy operation is finished.
device
    .queue_wait_idle(transfer_queue)
    .context("Failed to wait for the transfer to finish.")?;
```

After the copy operation is finished we have no more need for the transfer command buffer, nor for the staging buffer/memory so we free those. Note that we might want to keep these around for re-use in a bigger application but for now, we are just pushing the data to the GPU once and never changing it, so we can do that at the beginning an drop the staging buffers.

```rust
device.free_command_buffers(transfer_command_pool, &[command_buffer]);
device.free_memory(staging_buffer_memory, None);
device.destroy_buffer(staging_buffer, None);
```

---
# Modifying the Vertex Shader Function #

Finally, we can go ahead and just modify the create_vertex_shader function to simply use the staged buffer function we just created, with a buffer usage type of **VERTEX_BUFFER**. Since the function is so simple I'll just drop all the code here.

```rust
fn create_vertex_buffer(
    instance: &Instance,
    device: &Device,
    physical_device: PhysicalDevice,
    vertices: &[Vertex],
    transfer_command_pool: CommandPool,
    transfer_queue: Queue,
) -> Result<(Buffer, DeviceMemory)> {
    create_staged_buffer(
        instance,
        device,
        physical_device,
        vertices,
        BufferUsageFlags::VERTEX_BUFFER,
        transfer_command_pool,
        transfer_queue,
    )
    .context("Failed to create a vertex buffer.")
}
```

---
# Testing #

We'll go ahead and test that after making tese changes our program still works as before.

![Multicolor Square](Output%20From%20Staging%20Buffer.png)

Indeed it does! (and now our vertex data resides in the much faster GPU-only memory section :D)